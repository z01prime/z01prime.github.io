<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>注意力机制 | z01prime</title><meta name="author" content="z01prime"><meta name="copyright" content="z01prime"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Attention in transformers, step-by-step | Deep Learning Chapter 6 How might LLMs store facts | Deep Learning Chapter 7 重要的概念query、key和value。这三个词翻译成中文就是查询、键、值。 可以理解为一种行为：当厨师在锅里准备放下一个调料时，他会看一眼菜谱。书会将他的查询">
<meta property="og:type" content="article">
<meta property="og:title" content="注意力机制">
<meta property="og:url" content="http://example.com/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html">
<meta property="og:site_name" content="z01prime">
<meta property="og:description" content="Attention in transformers, step-by-step | Deep Learning Chapter 6 How might LLMs store facts | Deep Learning Chapter 7 重要的概念query、key和value。这三个词翻译成中文就是查询、键、值。 可以理解为一种行为：当厨师在锅里准备放下一个调料时，他会看一眼菜谱。书会将他的查询">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp">
<meta property="article:published_time" content="2026-01-24T02:30:50.000Z">
<meta property="article:modified_time" content="2026-02-01T08:27:13.281Z">
<meta property="article:author" content="z01prime">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":700},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '注意力机制',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2026-02-01 16:27:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 传送门</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://source.fomal.cc/img/dm8.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="z01prime"><span class="site-name">z01prime</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 传送门</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">注意力机制</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-24T02:30:50.000Z" title="发表于 2026-01-24 10:30:50">2026-01-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-01T08:27:13.281Z" title="更新于 2026-02-01 16:27:13">2026-02-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/">时间序列预测</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="注意力机制"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=eMlx5fFNoYc&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=7">Attention in transformers, step-by-step | Deep Learning Chapter 6</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9-Jl0dxWQs8&amp;t=1190s">How might LLMs store facts | Deep Learning Chapter 7</a></p>
<h1 id="重要的概念"><a href="#重要的概念" class="headerlink" title="重要的概念"></a>重要的概念</h1><p><strong>query、key和value</strong>。这三个词翻译成中文就是<strong>查询、键、值</strong>。</p>
<p>可以理解为一种行为：当厨师在锅里准备放下一个调料时，他会看一眼菜谱。书会将他的查询<strong>query</strong>映射到书中相关的标签<strong>key</strong>，如辣椒，盐等等，然后书中会展示最匹配的调料<strong>value</strong>。</p>
<p><strong>Source</strong> 和 <strong>Target</strong>。<strong>Source（源）</strong>指的是输入。这里是菜谱。<strong>Target（目标）</strong>指的是输出。这里是锅里正在炒的菜（以及厨师脑子里已经生成的做菜步骤）</p>
<p><strong>Self-Attention</strong>与<strong>Attention</strong>。上述的行为其实是属于 Attention（或者叫 Cross-Attention），而不是 Self-Attention。Attention 是拿着钥匙（Query）去开别人家的锁（Key）。</p>
<p>Self-Attention 是“把家里所有的零件（Q, K, V）摆在地上，看看谁和谁能拼在一起。</p>
<p>Cross-Attention可以理解为厨师做菜，Source是菜单/菜谱。Target是厨师锅里的菜。Cross-Attention 发生的过程就是当厨师（模型）在锅里准备放下一个调料（生成下一个词）时，他会抬头看一眼菜单（Source）。Query是厨师的询问，说明现在锅里有鸡肉了，下一步该放什么？Key是菜单上的每一行标题（如“主料“，调味”）。Value是菜单上对应的具体内容（如“葱段”、“盐”）。最终厨师通过 Query 去匹配菜单的 Key，最后拿到了 Value 放入锅中。这就是Target（锅里的进展）对 Source（静态菜单）的注意力。</p>
<p>厨师阅读菜单的时候，发生的是Self-Attention。菜单上写着“将洗净的<strong>辣椒</strong>切段，因为它很<strong>辣</strong>”。模型为了理解这个“它”是指辣椒还是指手，会自动把“它”和“辣椒”关联起来。这是Source 内部元素之间的互看。“它”产生的 Query 向量，在空间中的方向会指向“寻找那些具有名词属性且容易辣的东西”。而“辣椒”产生的 Key 向量，正好在空间中的那个位置。</p>
<h1 id="注意力机制的直观理解"><a href="#注意力机制的直观理解" class="headerlink" title="注意力机制的直观理解"></a>注意力机制的直观理解</h1><p>接下来的学习会以文本生成为例。</p>
<h2 id="Token"><a href="#Token" class="headerlink" title="Token"></a><strong>Token</strong></h2><p>指的是文本被切割成的小块，一般是单词或者是单词片段。</p>
<p>注意，Token 不是由Transformer模型本身分割的，而是由一个专门的预处理工具——分词器（Tokenizer）完成的。</p>
<p>分词器是在数据进入模型之前独立工作的。先把长文本切成标准规格的小块，再送进Transformer。</p>
<p>现在的主流模型（如 GPT-4, Llama）既不按字符切，也不完全按单词切，而是采用一种折中的方案。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>例子：unhappiness</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Word-level (按词)</strong></td>
<td><code>unhappiness</code></td>
<td>含义明确</td>
<td>词表太大（几百万个词），遇到新词（如网络流行语）就傻眼。</td>
</tr>
<tr>
<td><strong>Character-level (按字符)</strong></td>
<td><code>u, n, h, a, p...</code></td>
<td>词表极小（几十个字母）</td>
<td>字符本身没意义，序列太长，增加模型计算负担。</td>
</tr>
<tr>
<td><strong>Subword-level (子词 - 主流)</strong></td>
<td><code>un, happi, ness</code></td>
<td>兼顾两者</td>
<td>将常见词保留完整，生僻词拆成碎块。</td>
</tr>
</tbody>
</table>
</div>
<p>这个主流分法有点像词根词缀的分法，但是这其实是一种统计意义上的分法，在结果上确实表现得非常像“词根词缀”拆解法。人类看到 unhappiness，我们知道 un- 是否定前缀，happy 是词根，-ness 是名词后缀。而分词器是扫描了数以亿计的文本，发现 un 这个组合出现的频率高得离谱，ness 也是。于是它会认为两个碎块很有用，要把它们存进词表里。</p>
<p>所以这种分法往往与语言学结构高度吻合。但是事实上，如果the经常出现在there里，而re也是高频碎块，它可能会把there拆成the和 re。这在语言学上是错的，但对模型来说，只要能降低词表大小且覆盖所有字符就行。</p>
<p>用到了分词主要是为了建立数学联系：当两个向量在空间里靠得近，它们就产生了注意力。</p>
<h2 id="将Token关联到嵌入向量的高维向量。"><a href="#将Token关联到嵌入向量的高维向量。" class="headerlink" title="将Token关联到嵌入向量的高维向量。"></a>将Token关联到嵌入向量的高维向量。</h2><p>简单来说，嵌入向量（Embedding Vector）就是单词在模型中的表示，模型会把所有的单词转换为坐标。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123192231878.png" alt="image-20260123192231878" style="zoom:50%;"></p>
<p>在这一张图中，每一个单词都被转换成了上千个数字，每一个数字都可以被理解为一个属性维度，而大小则可以被理解为它们代表了在某个抽象维度上的偏移。虽然模型内部的维度很难用人类语言精准描述，但我们可以为了直观理解进行类比。假设一个只有 3 维的嵌入向量：一维表示的是是否是生物（-1到1），二维是体积大小，三维是是否昂贵。那么有：</p>
<ul>
<li>猫”可能是[0.9, -0.5, -0.2]</li>
<li>“狗” 可能是[0.9, -0.1, -0.3]（因为都是生物，第一项很接近）</li>
<li>“手机”可能是[-0.8, -0.6, 0.7]（非生物，且比较贵）</li>
</ul>
<p>而Transformer会计算这些向量之间的角度。如果两个向量指向的方向差不多，模型就认为它们之间需要产生强烈的注意力。</p>
<p>这个向量是怎么来的？它是练出来的。模型刚开始完全是乱猜的，给每个词分配一串随机数字。模型在读了海量文本后发现，猫和狗经常出现在宠物、吃、可爱这种词周围。为了让预测更准，模型会自动调整数字，让猫和狗的向量在坐标系里靠得非常近。</p>
<p>注意力模块不仅细化了一个词的含义，还允许模型互相传递这些嵌入向量所蕴含的信息。</p>
<p>在没有注意力模块之前，每个词的向量是孤立的。比如单词 Bank。训练好的向量包含了一点 银行 的意思，也包含了一点 河岸 的意思，是个模糊的混合体。注意力模块的作用是，如果周围的词有 Money 、 Interest ，注意力模块会把这些词向量中的金融特征提取出来，将这些信息组合为向量，加到 Bank 的向量中。此时 Bank 的向量被在空间中的位置发生了偏移，变成了一个纯粹代表 银行 的向量。这个例子中的K Q V：</p>
<p>Query (Q) : Bank发出了信号，需要知道Bank在这个上下文中的具体意义，在数学意义上，Q是针对当前词想要关注的方向。</p>
<p>Key (K)：是Money/Interest的标签，是由由周围的词（如 Money）产生。数学上K用来和Q做点积。如果Q（想要钱的信息）和K（我是钱）匹配度高，计算出来的注意力权重（Score）就会非常大。</p>
<p>Value (V)：Money/Interest 传递的实质信息。既然匹配上了，Money 就把它的“金融属性”打包成一个向量发过去。这是最终被融合到 Bank 向量上的那部分实质内容。</p>
<p>注意，<strong>单纯的词嵌入向量是不含位置信息的</strong>。而在实际的预测过程中，模型还会加上位置向量，代表了这个Token在一个句子中的位置，因为如果只给模型一堆词向量，模型分不清“狗咬人”和“人咬狗”，因为对模型来说这只是两个完全一样的向量集合。所以在这些词向量进入注意力模块之前，模型会给每个向量强行加上一个微小的、特制的位置向量（Positional Encoding）。比如I saw a saw.第一个saw是动词，而第二个就是名词，会有区别，他们的数值就会产生细微差别。这样模型就能通过数学计算知道这个词应该出现在哪一个位置。</p>
<p>有了这些带有位置和基础语义的向量，就可以通过<strong>$W_Q, W_K, W_V$</strong>矩阵进行线性变换。</p>
<h2 id="W-Q-矩阵"><a href="#W-Q-矩阵" class="headerlink" title="$W_Q$矩阵"></a>$W_Q$矩阵</h2><p>$W_Q$ 是一个具体任务。Transformer 并不是只有一个 $W_Q$，它拥有成百上千个并行的微型任务。因为Transformer是多头注意力机制，所以第一个头可能在问这个词的前面形容词的个数，另一个可能是在问前面名词个数，这里说是任务，实际上在数学中，他更像是一个视角，他放大了这个词的一些属性。在训练时，它是动态的。 模型通过不断看大量的句子，利用梯度下降算法去反复调整 $W_Q$ 矩阵里的每一个数字。在推理/使用时，它是“静态”的。一旦模型练好了，这个 $W_Q$ 矩阵里的数值就固定死了。</p>
<h2 id="W-K-矩阵"><a href="#W-K-矩阵" class="headerlink" title="$W_K$矩阵"></a>$W_K$矩阵</h2><p>$W_K$ 的作用就是将句子中每一个词的原始向量，也转换到一个同样的抽象任务空间里，把它变成一张方便被索引的名片（Key）。$W_Q$（Query）给出了一个具体任务，那么$W_K$（Key）就是每一个Token这个任务的具体的权重。具体来说，以“狗”这个词为例：</p>
<p>Embedding：它是“狗”这个词的各种属性分解，可能包含“动物”、“尺寸”、“寿命”、“忠诚度”等属性，将他压缩成了一串数字，称为向量。</p>
<p><strong>$W_Q$</strong>：它不是在制造属性，而是在筛选属性。它定义了在当前的注意力头里，我们只关心动物性和位置。它把这两者的权重调高，让 $Q$ 变成了一个带有强烈搜索指向的向量。</p>
<p><strong>$W_K$</strong>：它让所有的词（不仅仅是狗）都根据 $W_Q$ 的偏好来重新表达。它把所有词的 Embedding 转化为不同的 $K$，这个 $K$ 重点突出了“我是动物”这个标签，以便能被 $Q$ 发现。</p>
<p><strong>$Q \cdot K$</strong>：这步运算就是属性吻合度检测，当需求（Q）和拥有（K）在某些维度上同时呈现出高分，点积就会变得巨大。这个分值就是“注意力权重”。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123213100771.png" alt="image-20260123213100771" style="zoom:50%;"></p>
<p>这张图是假设$W_Q$为：有没有哪一个形容词在我前面。就会产生这样的结果，相乘结果越大说明得分越高，也就是越相关。</p>
<p>为什么要搞得这么复杂？既然 Embedding 里已经有“动物”属性了，为什么不能直接拿来比？答案是为了上下文灵活性。在“狗咬人”里，模型需要注意狗的“攻击性”。在“狗很可爱”里，模型需要注意狗的“外观属性”。同一个 Embedding，通过不同的 $W_Q$ 和 $W_K$（不同的头），可以在第一种情况下提取“攻击性”进行匹配，在第二种情况下提取“可爱度”进行匹配。</p>
<p>这样模型就能够根据语境理解单词。</p>
<p><strong>$W_Q$</strong> 定义了“任务”：在这个视角下，哪些属性是加分项。</p>
<p><strong>$W_K$</strong> 提取了“名片”：把词的原始含义转化成该视角下的属性值。</p>
<p><strong>$Q \cdot K$</strong> 实现了“匹配”：属性越吻合，分值越高，产生的权重就越大。</p>
<p>回到那张图，然后将这些经过SoftMax后得到：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123213845905.png" alt="image-20260123213845905" style="zoom:67%;"></p>
<p>这个叫做注意力模式(Attention Pattern)，在论文中被表示为一个公式：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123214116705.png" alt="image-20260123214116705" style="zoom:67%;"></p>
<p>这里面有一个V，先不考虑，$\sqrt{d_k}$被称为缩放因子（Scaling Factor），它的出现是为了解决一个数学问题：防止梯度消失。</p>
<p>$d_k$ 代表Key向量的维度。简单来说，就是向量里有多少个数字。</p>
<p>要除以它的平方根是因为Softmax对巨大的数值非常不敏感，点积会随着维度变大而爆炸，假设向量里的每个数字平均大小是 1。如果向量只有 2 维，点积结果大概是 2；但如果向量有 1000 维，点积结果可能就会冲到 1000 左右。Softmax 函数在输入数值非常大（比如 100 或 1000）时，其梯度（导数）会变得极其微小，几乎接近于 0。如果分数太高，Softmax 算出来的概率会变成极其极端的 $[0.9999…, 0.0000…]$。一旦进入这个状态，模型在训练时就无法通过梯度来更新权重了。也叫梯度消失。通过除以 $\sqrt{d_k}$，模型强行把那些可能“爆炸”的点积得分拉回到一个适中的范围。</p>
<h2 id="掩码"><a href="#掩码" class="headerlink" title="掩码"></a>掩码</h2><p>此外还有一个细节，在训练的过程中，对于给定的示例文本训练时，模型会预测出下一个词的概率高低，然后最小化损失函数来调整权重，但是更效率的做法是同时预测每一个初始Token子序列的下一个Token，比如可以同时预测第一个‘the’的后一个Token，也可以预测‘creature’的下一个Token，这样每一个训练样本可以提供多次训练机会，这也是Transformer训练比老一代模型（如 RNN）快得多的原因：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124104808697.png" alt="image-20260124104808697" style="zoom:67%;"></p>
<p>但是存在一个问题，我们需要让模型看不到后面的词，不然后面的词会影响前面的词，存在数据泄漏。所以在softmax之前，往往会将后面的值初始化为负无穷：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124105219170.png" alt="image-20260124105219170" style="zoom:50%;"></p>
<p>这在训练阶段是必须防止的。但其实在测试/生成阶段（比如和已经训练好的模型聊天时），其实不需要专门设置负无穷，因为那时候未来的词根本还没生出来，模型想抄也抄不到。GPT 系列 (ChatGPT, Llama, Claude)这类模型会在两个阶段都设置掩码，但是像是BERT这类模型在任何阶段都不使用这种防偷看的掩码。</p>
<p>这个是在自注意力机制下的，如果是交叉注意力，那么其实根本就不用管掩码的问题。</p>
<h2 id="W-V-矩阵"><a href="#W-V-矩阵" class="headerlink" title="$W_V$矩阵"></a>$W_V$矩阵</h2><p>然后$V$（Value）其实就是负责怎么把实质内容搬运过去的指令集。</p>
<p>为什么不直接加原向量，而要用 $V$ 转换一下？</p>
<p>$V$ 的作用是告诉模型应该搬运什么信息。它能够过滤杂质，因为原始的 Embedding $\vec{E}$ 包含了太多的信息（比如这个词的词性、位置、首字母是什么等）。但在当前的注意力任务中（比如“寻找前面的形容词”），<code>creature</code> 只需要 <code>blue</code> 这个词中关于“蓝色”的具体语义。$W_V$ 矩阵也是一个信息提取器。它把 <code>blue</code> 的原始向量中那些对当前任务有意义的实质性内容提取出来，变成了 $\vec{V}$。</p>
<p> $V$ 与 $K$ 的区别，其实是标签与内容的区别，具体来说$K$ (Key)：是给别人看的，为了让 $Q$ 找到自己，它决定了权重是多少。$V$ (Value)：是给别人用的（为了让 $Q$ 吸收自己），它决定了搬运的内容是什么。比如去买书，书的封面和标题是 $K$，它是吸引买家买下它的理由（产生权重）；书里的文字内容是 $V$，他是最后带回家读进脑子里的东西。数学上是加权求和实现的，$Q$ 和 $K$ 算出了权重（比如 $0.58$）。通过$W_V$ 把该词变成 $\vec{V}$。然后将 $\vec{V}$ 乘以 $0.58$。最后把这个缩放后的向量加到产生 $Q$ 的那个词（<code>creature</code>）的 $\vec{E}_4$ 向量上。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123224816601.png" alt="image-20260123224816601" style="zoom: 50%;"></p>
<p>在自注意力机制中，每一个词都在同时扮演“提问者”和“被选者”的角色。在这一层处理结束时，不仅仅是 <code>creature</code> 变聪明了，句子里的每一个词都根据上下文完成了更新：</p>
<ul>
<li><strong>对于 <code>creature</code></strong>：它吸收了 <code>fluffy</code> 和 <code>blue</code> 的信息，变成了“蓬松的蓝色的生物”。</li>
<li><strong>对于 <code>forest</code></strong>：它可能通过自己的 $Q$ 找到了前面的 <code>verdant</code>（翠绿的），从而吸收了“绿色”的特征，变成了“翠绿的森林”。</li>
<li><strong>对于 <code>roamed</code></strong>：它可能关注到了 <code>creature</code>，从而在向量里补充了“是谁在漫游”的信息。</li>
</ul>
<p>这种全员同步更新的过程，就是图中下方那一排 $\Delta \vec{E}_1, \Delta \vec{E}_2 \dots$ 的含义。每一个词都算出了一个所有词的权重，然后加回到自己原来的向量上，生成了全新的 $\vec{E}’$。</p>
<p>注意这个是单头注意力的情况，它只代表了模型一种观察句子的方式，但是如果只用这一个矩阵，模型可能就没精力去管“主谓关系”或“代词指代”了。这就是为什么真正的 Transformer 是多头的：它们会并行运行好几个这样的内容，每个头都有自己的 $W_Q, W_K, W_V$。比如头 1让名词找形容词；头 2 让动词找主语；头 3找逻辑转折词。</p>
<p>右侧公式$\vec{E}_n + \Delta \vec{E}_n = \vec{E}’_n$中，</p>
<ol>
<li>$\vec{E}_n$ (原始值)：保持了词语原本的含义（比如“我是个生物”）。</li>
<li>$\Delta \vec{E}_n$ (变化量)：这是通过刚才那一堆 $Q, K, V$ 计算出来的上下文中需要注意的内容。</li>
<li>$\vec{E}’_n$ (最终结果)：这是带有上下文理解的词向量。</li>
</ol>
<p>这种“加法”操作在深度学习里叫残差连接（Residual Connection）。它的好处在于即便注意力机制算错了，原始的词意也不会丢失，模型只会把注意力看作一种“补充修正”。这个概念最早来自 2015 年的 ResNet 论文。在数学上，如果你想让一个函数 $f(x)$ 学习某种变换，直接学 $f(x)$ 可能很难。但如果我们将输出定义为 $H(x) = x + f(x)$，那么 $f(x)$ 只需要学习输出与输入之间的差值（即 $H(x) - x$）。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095005646.png" alt="image-20260125095005646" style="zoom:50%;"></p>
<p>以GPT-3为例，每个模块内使用96个注意力头，即有96个不同的键和查询矩阵，会产生96种注意力模式，每个注意力头都会有独特的值矩阵，用于产生96个值向量序列，这张图中的中间部分展示了每一个token，每个头都会给出要加入到这个位置的嵌入中的变化量，所以对于原始的嵌入，要加上所有注意力头给出的变化量。</p>
<p>在具体实现中，还有一个细节，之前提到为了增加运行效率，可以对这个大矩阵进行低秩分解：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124114735640.png" alt="image-20260124114735640" style="zoom:50%;"></p>
<p>简单来说，模型并没有直接使用一个包含 $1.5$ 亿参数（$12,288^2$）的大矩阵，而是把它拆成了两个“瘦长”的矩阵相乘：</p>
<ul>
<li><strong>矩阵 A ($Value_\downarrow$)</strong>：大小为 $128 \times 12,288$（将高维压缩到低维）。</li>
<li><strong>矩阵 B ($Value_\uparrow$)</strong>：大小为 $12,288 \times 128$（将低维重新映射回高维）。</li>
</ul>
<p>$W_V$ 的效果 $\approx$ 矩阵 B $\times$ 矩阵 A，这种低秩分解（Low-rank Decomposition）和 SVD（奇异值分解）有点像，不同的是SVD 分解：通常是指已经有一个现成的大矩阵，为了分析它或者压缩它，用数学手段把它拆成三个矩阵的乘积 ($A = U\Sigma V^T$)。这会保留前 $k$ 个最大的奇异值，从而得到一个低秩的近似矩阵。而Transformer 中的低秩设计是，模型在设计之初，就根本没打算创建一个完整的大矩阵。工程师直接让模型去学习两个小的瘦长矩阵（矩阵 A 和 矩阵 B）。因此对每一个注意力头的$W_V$都做了一个分割：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095036229.png" alt="image-20260125095036229" style="zoom:50%;"></p>
<p>但是实际实现中$Value\uparrow$和$Value\downarrow$的说法会有所变化，所有的$Value\uparrow$会合在一起说成是输出矩阵，与整个注意力模块相关联，而每一个注意力头的$W_V$其实是$Value\downarrow$矩阵，即那些将嵌入向量投影到低纬度空间的矩阵。</p>
<h1 id="多层感知机-前馈神经网络"><a href="#多层感知机-前馈神经网络" class="headerlink" title="多层感知机/前馈神经网络"></a>多层感知机/前馈神经网络</h1><p>这部分占了大部分的模型参数。其实和传统的模型差不多，大致意思就是把信息打散了存在某一块里面。</p>
<p>有理论认为，这一块提供了额外的容量来存储事实。</p>
<p>以“迈克尔乔丹打篮球”这一事实为例，假设这个高维空间里面，有一个向量能够表示名字是迈克尔，另外有一个向量表示姓氏是乔丹，并且这两个向量几乎是垂直的。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095423417.png" alt="image-20260125095423417" style="zoom:50%;"></p>
<p>视频里面说垂直的原因在于，在高维空间中，无数个互不干扰的方向，非常空旷，而随机抽取两个向量，它们之间夹角接近 $90^\circ$ 的概率极高。显然，对于一个一个N维空间，最多只有N个两两垂直的向量，但是放宽一点限制，比如89°、91°也算正交，那么这些向量的数量是随维度的增加指数级增长的。这被称为高维空间的近似正交性，Johnson-Lindenstrauss (JL) 引理：在 $N$ 维空间中，你可以找到多达 $e^{N \cdot \epsilon^2}$ 个向量，它们两两之间的夹角都在 $90^\circ \pm \epsilon$ 之间。所以当从高维空间随机采样向量时，这些向量的分量往往表现出正态分布的特性，而这个特性会导致近似正交，证明：</p>
<p>因为在 $N$ 维空间有两个随机单位向量 $\vec{A}$ 和 $\vec{B}$。它们的内积（点积）决定了它们是否交：</p>
<script type="math/tex; mode=display">
\vec{A} \cdot \vec{B} = \sum_{i=1}^{N} a_i b_i</script><p>由于每个分量 $a_i$ 和 $b_i$ 都近似服从均值为 0 的正态分布：</p>
<ol>
<li>每一项 $a_i b_i$ 也是一个随机变量，其均值依然是 0。</li>
<li>当把这 $N$ 个项加起来时，根据大数定律，正值和负值会极大概率相互抵消。</li>
<li>随着维度 $N$ 越来越大，这个总和（内积）会极其趋近于 0。</li>
</ol>
<p>而内积趋近于 0，意味着夹角趋近于 $90^\circ$。</p>
<p>其实在机器学习中，通常都是希望模型学到的特征是独立且不相关的。这个可以说是模型训练后的理想结果；此外，模型在梯度下降时，如果发现两个概念混淆导致了误差，它也会这两个向量在空间中尽可能的正交，直到它们互不干扰（即趋向垂直）。这也是为什么再后面降到MLP的具体设计中，要把词向量进行升维。</p>
<p>如果有一个向量，表示迈克尔乔丹这个人，那么他与姓名这两个向量的点积一定是1（这里是假设1表示完美的信号匹配，在真实的 Transformer 模型中，点积经常大于 1，如果点积大于 1，通常意味着该特征被强化了，这也是为什么注意力机制公式里会看到 $\frac{QK^T}{\sqrt{d_k}}$。那个分母 $\sqrt{d_k}$ 就是为了把可能由于高维产生的巨大点积数值“压”回来，防止它太大导致 Softmax 饱和）。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095438986.png" alt="image-20260125095438986" style="zoom:50%;"></p>
<p>此外还有一个假设：由于“迈克尔·乔丹”这个文本跨越了两个Token（一个是左边的迈克尔，另一个是右边的乔丹），他要假设前面的注意力模块，成功将信息传递给这两个向量中的第二个（乔丹模块）确保能编码全名。</p>
<p>这个假设实际上是在解释模型如何处理分布式特征。在文本中，“Michael” 和 “Jordan” 是两个独立的 Token。在第一层，Michael 对应的向量只知道自己是“迈克尔”，Jordan 对应的向量只知道自己是“乔丹”。而MLP 层是逐位置计算的，它不会看旁边的词。如果信息不传递，Jordan 位置的 MLP 永远不知道前面站着一个 Michael，也就无法触发“篮球飞人”这个特定事实的提取。因此，必须有一个前置步骤，把分散的信息“汇聚”到一个地方。</p>
<p>在MLP里面，每一个向量都会通过一些操作，最终会得到另一个维数相同的向量，将得到的向量与原本的向量增加，而增加的结果就是输出值。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095451202.png" alt="image-20260125095451202" style="zoom:50%;"></p>
<p>注意这个时候，向量之间是不会交流的。</p>
<p>在刚才的例子中，注意力模块发现“乔丹”和“迈克尔”关系紧密，于是把“迈克尔”这个向量的信息拷贝并叠加到了“乔丹”的向量上。得到了一个编码了“名字迈克尔”和“姓氏乔丹”的向量，这个向量流入MLP，经过这一系列的运算，能够输出包含“篮球”方向的向量，再将其与原向量相加，就会得到输出向量。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095500370.png" alt="image-20260125095500370" style="zoom:50%;"></p>
<p>MLP 的第一层矩阵是线性变换 (Linear)。其中某一个神经元（一组权重）专门在等一个特定的信号：即“名=迈克尔”且“姓=乔丹”。如果这个混合向量里的两个信号同时足够强，点积结果就会很大，从而通过 ReLU 这个函数激活。一旦被激活了，MLP 的第二层线性层就会输出一个全新的向量。这个向量的方向代表了“篮球”这个概念。</p>
<p>最后一步就是残差的相加”：</p>
<script type="math/tex; mode=display">
\vec{E}_{new} = \vec{E}_{Michael+Jordan} + \Delta\vec{E}_{Basketball}</script><p>模型并没有把“乔丹”变成“篮球”，而是在“乔丹”的向量里添加了“篮球”这个属性。而经过这一层处理，这个向量就像变成了一个“带有篮球运动员标签的乔丹”。这个进化的向量会被传送到下一层，去寻找更多的关联（比如预测下一个词会不会是“灌篮”）。</p>
<p>所以在论文图中也有看到，其实在经过注意力和前馈神经网络（MLP）之后都有一个“Add”的操作，两者的区别在于Attention它打破了原本词向量间的孤立，让“乔丹”向量拿到了“迈克尔”的信息。MLP它识别出这个组合，产出对应的“事实”向量。而残差连接能把事实写回向量中。</p>
<p>注意力解决了 “谁和谁有关” 的问题。它把分散在序列各处的信息聚拢到一个向量里。MLP 解决了 “这些信息合在一起意味着什么” 的问题。它从聚拢的信息中提取出深层的、隐含的事实（比如“篮球”）。</p>
<p>其实MLP的结构和以前的传统神经网络很像，其实就是标准的三层架构：</p>
<ul>
<li>输入层：维度为 $d_{model}$。</li>
<li>隐藏层：通过线性变换（Linear）将维度拉伸到 $d_{ff}$，然后接一个非线性激活函数（ReLU 或 GELU）。</li>
<li>输出层：再通过一个线性变换缩回到 $d_{model}$。</li>
</ul>
<p>以前的神经网络通常尝试用一个巨大的MLP完成所有任务（识别、理解、输出）。Transformer 里的 MLP不再负责寻找词与词的关系，而是专心做存储。这也是为什么都在说MLP负责存储在训练中获得的知识。</p>
<p>具体来说，第一层就是一个线性层，就是传统 BP神经网络 的核心逻辑类似。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125100716255.png" alt="image-20260125100716255" style="zoom:50%;"></p>
<p>MLP 的内部计算公式是标准的 $W \vec{E}_i + \vec{B}$：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095520539.png" alt="image-20260125095520539" style="zoom:50%;"></p>
<p>然后就是传统的神经网络模型，用ReLu将线性表现为非线性：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095528883.png" alt="image-20260125095528883" style="zoom:50%;"></p>
<p>当然在大多数模型中，会用一个叫做GeLu(高斯误差线性单元)：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095540470.png" alt="image-20260125095540470" style="zoom:50%;"></p>
<p>因为ReLU (Rectified Linear Unit)的逻辑非常死板。只要点积结果是 $-0.00001$，输出就是 $0$；只要是正数，就原样输出。那么这样在训练中容易出现“神经元死亡”现象：如果一个神经元的偏置项变成了很大的负数，它可能永远无法被激活，导致这部分“内存”废掉了。而GeLu它结合了概率的思想。当输入是负数但接近 $0$ 时，它不会立刻切断信号，而是允许一点点带权重的负值信号（抑制信号）传过去。在处理“迈克尔·乔丹”这样的复杂语义时，GELU 的表现更像人类的模糊逻辑，现实中的向量信号不会像假设的“刚好是 1”那么完美，GELU 允许模型在信号稍有偏差或带有轻微噪音时，依然能保留一部分有用的信息，而不是直接把它杀掉。</p>
<p>经过这一部分的处理之后，最后一步是将这个向量降维至原始维度。在第一层（Up-projection）之后，向量被拉伸到了极高的维度（如 51,200 维），这是为了提供足够的检测空间。所以我们需要一个巨大的 $12,288 \times 51,200$ 的矩阵，把这 5 万多个神经元的输出“加权求和”，压缩回原始的 12,288 维。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125100933460.png" alt="image-20260125100933460" style="zoom:50%;"></p>
<p>在训练初期，这个矩阵里的数值也是随机的乱码。最初，当“迈克尔·乔丹”神经元被激活时，这个降维矩阵可能随机地输出一个代表“厨师”或“天空”的向量。模型发现预测错了，梯度信号就会顺着路径传回来，修改这个降维矩阵里对应那一列的数值。经过无数次修正，那一列的数值慢慢变成了一个指向篮球语义的向量。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125101649720.png" alt="image-20260125101649720" style="zoom:50%;"></p>
<p>这个矩阵的列数和中间隐藏层的神经元数量一样多（比如 5 万列）。在这里，按列来看矩阵更符合知识提取的含义：</p>
<p>假设我们有一个矩阵 $A$（对应 MLP 的降维矩阵）和一个向量 $\vec{x}$（对应神经元的激活值）：</p>
<script type="math/tex; mode=display">
A\vec{x} = \begin{bmatrix} \vert & \vert & & \vert \\ \vec{C}_1 & \vec{C}_2 & \dots & \vec{C}_m \\ \vert & \vert & & \vert \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{bmatrix} = x_1\vec{C}_1 + x_2\vec{C}_2 + \dots + x_m\vec{C}_m</script><p>这个公式的意思是：矩阵 $A$ 乘以向量 $\vec{x}$，等于矩阵 $A$ 的各列向量以向量 $\vec{x}$ 的各个分量为系数进行线性组合。上图中可表示为：</p>
<script type="math/tex; mode=display">
\text{Output} = n_0\vec{C}_0 + n_1\vec{C}_1 + n_2\vec{C}_2 + \dots + \vec{B}</script><p>这里的每一个 $n_m$ 都是中间层神经元的激活值（也就是检测器的结果），而每一列 $\vec{C}_m$ 就是这个神经元对应的”结论向量”。更通俗一些可以理解为当多个神经元同时激活时，模型就把这些列向量按比例加在一起，比如：$0.8 \cdot [\text{篮球}] + 0.5 \cdot [\text{飞人}] + 0.2 \cdot [\text{棒球}]$，最终得到的 $\Delta\vec{E}$ 就是这些语义的“混合结论”。把这个矩阵记为$W\downarrow$偏置向量记为$B\downarrow$，然后把这些残差加回去，就表示模型学到了新的结论：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125103622914.png" alt="image-20260125103622914"></p>
<h1 id="GPT3的参数量计算"><a href="#GPT3的参数量计算" class="headerlink" title="GPT3的参数量计算"></a>GPT3的参数量计算</h1><p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124110107947.png" alt="image-20260124110107947" style="zoom: 50%;"></p>
<ol>
<li>词向量层 (Embedding &amp; Unembedding)，其实就是词典。</li>
</ol>
<ul>
<li>Embedding: 把文字（Token）变成初始向量。</li>
<li>Unembedding: 把最后算完的向量变回文字。</li>
<li>参数量: 取决于词库的大小（GPT-3 约有 5 万个词）和向量的维度（12,288 维）。这部分虽然重要，但在总参数量中占比很小。</li>
</ul>
<ol>
<li>注意力机制相关 (Key, Query, Value, Output)：GPT-3 有 96 层，每一层都有96 个注意力头。</li>
</ol>
<ul>
<li>Key, Query, Value ($W_K, W_Q, W_V$): 每一层、每一个头都有这三组矩阵。它们负责词与词之间的信息交换。</li>
<li>Output ($W_O$): 当 96 个头各自算出结果后，需要一个矩阵把这些不同的视角重新合并在一起。</li>
<li>每一层都有巨大的矩阵来处理 12,288 维的向量。<strong>因为每一层有96个头，所以每个头分配到的是12288/96=128,128×12288大小的$W_K, W_Q, W_V$</strong>。</li>
</ul>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124114526262.png" alt="image-20260124114526262" style="zoom:50%;"></p>
<p>上文也有提到，为了符合矩阵运算$W_V$应该是12288×12288,但实际上会对大矩阵进行低秩分解，做了一个分割：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095036229.png" alt="image-20260125095036229" style="zoom:50%;"></p>
<p>但是实际实现中$Value\uparrow$和$Value\downarrow$不叫这个名字，所有的$Value\uparrow$会合在一起说成是输出矩阵，与整个注意力模块相关联，而每一个注意力头的$W_V$其实是$Value\downarrow$矩阵，即那些将嵌入向量投影到低纬度空间的矩阵。所以真正的$W_V$矩阵是12288（词向量大小）× 每一个注意力头需要的维度（12288/96=128）。Output ($W_O$)的维度则是他的转置矩阵的维度。</p>
<ol>
<li>前馈神经网络 (Up-projection &amp; Down-projection)，在每一层注意力计算完之后，模型会进入一个“全连接层”（Feed Forward）。</li>
</ol>
<ul>
<li>Up-projection: 把向量维度拉伸得非常大（GPT-3 将 12,288 维拉伸到 49,152 维，乘上4倍）。这是模型推理的地方。</li>
<li>Down-projection: 再把维度压缩回 12,288 维，传递给下一层。</li>
<li>这是模型中参数量最大的部分。大约 2/3 的参数都集中在这里。</li>
</ul>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125104248008.png" alt="image-20260125104248008"></p>
<p>一个Token维度的变化：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>阶段</strong></th>
<th><strong>维度变化</strong></th>
<th><strong>具体的改变</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1. 初始输入</strong></td>
<td>$1 \to 12,288$</td>
<td><strong>查表嵌入（Embedding）</strong>。一个单词 ID 变成了一个 1.2 万维的稠密向量。</td>
</tr>
<tr>
<td><strong>2. 注意力层</strong></td>
<td>保持 $12,288$</td>
<td><strong>信息的交流</strong>。向量通过 Attention 吸收了周围词的信息，但体型没变。<br>具体来说，一开始是12,288 维的 $\vec{E}_i$。<br>然后被96个注意力头分身成 96 对 $\vec{Q}, \vec{K}, \vec{V}$，每组 128 维。<br>各组根据 $\vec{Q}$ 和 $\vec{K}$ 打分，确定注意力分数。<br>根据分数，并按照各自对应的$\vec{V}$ 加权求和，形成 96 个 128 维的$\Delta \vec{E}_i$。<br>把$\Delta \vec{E}_i$拼回 12288 维。<br>通过残差连接（Add），把这些情报加回原来的 $\vec{E}_i$ 上。</td>
</tr>
<tr>
<td><strong>3. MLP 升维</strong></td>
<td>$12,288 \to 49,152$</td>
<td><strong>升维（Up-projection）</strong>。向量被拉伸到近 5 万维，去触发那些“事实检测器”。</td>
</tr>
<tr>
<td><strong>4. 激活函数</strong></td>
<td>保持 $49,152$</td>
<td><strong>筛选（GELU）</strong>。5 万个维度中，只有少数符合逻辑的“神经元”保持活跃。</td>
</tr>
<tr>
<td><strong>5. MLP 降维</strong></td>
<td>$49,152 \to 12,288$</td>
<td><strong>降维（Down-projection）</strong>。把筛选出的新知识压缩回原始宽度。</td>
</tr>
<tr>
<td><strong>6. 循环往复</strong></td>
<td>重复以上过程 96 次</td>
<td><strong>迭代</strong>。GPT3设置了96层（n_layer参数），每一层都像这样升维再降维，向量就能获取更多的知识。</td>
</tr>
<tr>
<td><strong>7. 最终输出</strong></td>
<td>$12,288 \to 50,257$</td>
<td><strong>解密（Unembedding）</strong>。向量映射到 5 万个单词概率上，决定吐出哪个字。</td>
</tr>
</tbody>
</table>
</div>
<p>*其实这个对于GPT来说的<code>n_layer</code>参数在其他模型中叫做Decoder层的维度，传统 Transformer（如翻译任务）需要 Encoder 读入中文，Decoder 产出英文。GPT 系列是纯 Decoder 架构（Decoder-only），它本质上是一个“接龙”模型。它把之前写好的所有字都当成“已有的输入”，直接塞进一串 Decoder 里去预测下一个字。</p>
<h1 id="Transformer源码解读"><a href="#Transformer源码解读" class="headerlink" title="Transformer源码解读"></a>Transformer源码解读</h1><p>GPT-3是只用了Decoder，但是实际上在原始论文中，Transformer的架构是用于机器翻译的。找了一个Transformer的源码：<a target="_blank" rel="noopener" href="https://github.com/graykode/nlp-tutorial/tree/master">https://github.com/graykode/nlp-tutorial/tree/master</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/z01prime/TransformerCodeTutorial">更好的阅读体验/jupyteerNoteBook源码链接：z01prime/TransformerCodeTutorial: pytorch的Transformer的代码实现</a></p>
<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>windows 11</p>
<p>Python 3.11.14 </p>
<p>包环境：</p>
<ul>
<li>numpy 1.23.5</li>
<li>torch 2.2.2</li>
<li>matplotlib 3.7.0</li>
</ul>
<p>也没有这么严格的要求，能不冲突就行</p>
<h2 id="导入一些包"><a href="#导入一些包" class="headerlink" title="导入一些包"></a>导入一些包</h2><p>首先要先导入一些常用的库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>] = <span class="string">&quot;TRUE&quot;</span></span><br></pre></td></tr></table></figure>
<p>解决可能会有的老环境问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p>在 Transformer 中主要用于处理矩阵运算、生成位置编码（Positional Encoding）的三角函数序列，以及处理一些数组形状的变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>这个是PyTorch 的核心库，提供了张量操作，支持GPU加速</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure>
<p>PyTorch 的神经网络模块（neural network）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure>
<p>提供一些优化器，包含了如 Adam、SGD 等优化器，用于根据损失函数计算的梯度来更新模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>画图用的</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>因为是演示，所以简单提供一些数据就行。写的简陋些，就假设要训练一个德语翻译为英语的任务：ich mochte ein bier P翻译为i want a beer</p>
<p>代码里准备了三份数据：</p>
<p>enc_inputs: ich mochte ein bier P (Encoder 看到的原话)</p>
<p>dec_inputs: S i want a beer (Decoder 看到的“辅助信息”，告诉它：从 S 开始，后面跟着这些词)</p>
<p>target： i want a beer E (模型真正的“学习目标”，告诉它：最后要输出这些词，并以 E 结束)</p>
<p>这种机制在深度学习中被称为 Teacher Forcing（老师引导/强制教学）。dec_inputs和target有点像，因为dec_inputs是输入，用于给模型参考的。而target是目标，是标准答案。它们的关系是错开一位的：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>时间步 (Step)</th>
<th>Decoder 输入 (dec_inputs)</th>
<th>模型尝试预测的词</th>
<th>标准答案 (target_batch)</th>
</tr>
</thead>
<tbody>
<tr>
<td>第 1 步</td>
<td><strong>S</strong></td>
<td>$\rightarrow$</td>
<td><strong>i</strong></td>
</tr>
<tr>
<td>第 2 步</td>
<td>S <strong>i</strong></td>
<td>$\rightarrow$</td>
<td><strong>want</strong></td>
</tr>
<tr>
<td>第 3 步</td>
<td>S i <strong>want</strong></td>
<td>$\rightarrow$</td>
<td><strong>a</strong></td>
</tr>
<tr>
<td>第 4 步</td>
<td>S i want <strong>a</strong></td>
<td>$\rightarrow$</td>
<td><strong>beer</strong></td>
</tr>
<tr>
<td>第 5 步</td>
<td>S i want a <strong>beer</strong></td>
<td>$\rightarrow$</td>
<td><strong>E</strong></td>
</tr>
</tbody>
</table>
</div>
<p><code>dec_inputs</code> 总是比 <code>target</code> 早一个位置（这也是论文中的Transformer框架图里面为什么还注上了Shift Right）。模型在每个位置的任务都是：根据已经看到的词，预测下一个词。</p>
<p>这个是训练部分，当然在真实的测试场景根本没有target，因为还不知道答案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sentences = [<span class="string">&#x27;ich mochte ein bier P&#x27;</span>, <span class="string">&#x27;S i want a beer&#x27;</span>, <span class="string">&#x27;i want a beer E&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="词表与Target-Source长度"><a href="#词表与Target-Source长度" class="headerlink" title="词表与Target/Source长度"></a>词表与Target/Source长度</h2><h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><p>那么除了提供训练的句子，还要有一个词表用于分词，所以自己定义了一个词表:</p>
<p>这个是source的词表：<code>src_vocab = &#123;&#39;P&#39;: 0, &#39;ich&#39;: 1, &#39;mochte&#39;: 2, &#39;ein&#39;: 3, &#39;bier&#39;: 4&#125;</code></p>
<p>这个是Target的词表：<code>tgt_vocab = &#123;&#39;P&#39;: 0, &#39;i&#39;: 1, &#39;want&#39;: 2, &#39;a&#39;: 3, &#39;beer&#39;: 4, &#39;S&#39;: 5, &#39;E&#39;: 6&#125;</code></p>
<p><code>tgt_vocab</code>比<code>src_vocab</code>多了 ‘S’ 和 ‘E’。并且<code>src_vocab</code>有一个‘P’.因为<code>src_vocab</code>是给Encoder用的（德语），而<code>tgt_vocab</code>是给Decoder用的（英语）。</p>
<p>这些都是特殊字符：</p>
<p><code>S</code> (Start / BOS - Beginning of Sentence)，他是解码器的起始符号。在推理（翻译）时，解码器（Decoder）第一步并不知道该输出什么。我们会喂给它一个 <code>S</code>，告诉它从这里开始翻译第一个词。</p>
<p><code>E</code> (End / EOS - End of Sentence)，是句子的终止符号。训练时，当翻译完最后一个词后，模型要输出一个 <code>E</code>表示结束了。这样在实际使用时，看到模型输出了 <code>E</code>，就知道翻译结束了，否则它可能会无限循环下去。</p>
<p><code>P</code> (Padding)，就是一个填充符号。用于对齐。如果一个 Batch里，第一句有 5 个词，第二句只有 4 个词，我们会给第二句补上一个 <code>P</code>，让两句话长度一致，方便并行计算。具体来说，Padding 的目标是让整个 Batch 变成一个 矩形矩阵（$Batch_Size \times Seq_Len$）。这里‘p’是0是有原因的，如果位置向量是全 0，那么根据公式：<script type="math/tex">\text{Embedding}(P) + \text{Positional}(0) = \text{Embedding}(P)</script><br>这意味着填充位 ‘P’ 在空间中没有任何位置偏移，它不会携带任何顺序信息。</p>
<p>还有一个好处是掩码的时候算的很快，因为判断一个数是否等于 0 是最快的。之后代码中的<code>get_attn_pad_mask</code>函数会有用到。</p>
<p><strong>这里说的“词”都是指Token，而不是字符数或单词数，当然真实情况下分词器不一定就是恰好一个单词一个单词分，是按照每个字母组合的概率来分，分出来的有点像是词根词缀</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transformer Parameters</span></span><br><span class="line"><span class="comment"># Padding Should be Zero</span></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;ich&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;mochte&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;ein&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;bier&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;want&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;beer&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">6</span>&#125;</span><br><span class="line">number_dict = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125; <span class="comment"># 做一个反向的映射数字-&gt;单词</span></span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br></pre></td></tr></table></figure>
<p><code>number_dict</code>用于解码，把模型输出的数字，翻译回人能看的单词。</p>
<p>比如： [1, 2, 3, 4, 6] → i want a beer E</p>
<h3 id="Target-Source长度"><a href="#Target-Source长度" class="headerlink" title="Target/Source长度"></a>Target/Source长度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">src_len = <span class="number">5</span> <span class="comment"># length of source</span></span><br><span class="line">tgt_len = <span class="number">5</span> <span class="comment"># length of target</span></span><br></pre></td></tr></table></figure>
<p>还要设置source和target的长度，这里统一设置为5</p>
<p>比如传进去的<code>batch_size</code>参数设为2，那么比如有这样的句子：</p>
<p>Gib mir ein Glas Wasser</p>
<p>ich mochte ein bier</p>
<p>规定按照单词来分词的话，那么显然第一句和第二句的长度不一样，第二句少了一个Token，要补一个P，这样让两句话长度一致，方便并行计算。所以这也是为什么我给的句子中是“ich mochte ein bier P”而不是“ich mochte ein bier”</p>
<p>在实际的大规模模型（如 GPT 或 BERT）中，一般会使用专门的 Tokenizer（如 Byte-Pair Encoding, BPE）。但在这段代码里，我使用的是最原始的空格切分：<code>input_batch = [[src_vocab[n] for n in sentences[0].split()]]</code>。</p>
<p>在时间序列任务中，为了让 Decoder 更有序地预测，通常会给它一段已知的历史数据作为前缀（即<a target="_blank" rel="noopener" href="https://github.com/thuml/Time-Series-Library">https://github.com/thuml/Time-Series-Library</a> 启动脚本中的label_len部分），这和 NLP 的 Teacher Forcing 非常像。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>时间序列参数</th>
<th>含义</th>
<th>对应代码中的部分</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>seq_len</code></td>
<td>历史观察窗口长度 (Look-back window)</td>
<td><code>src_len</code> (Encoder 输入的长度，这里是 5)</td>
</tr>
<tr>
<td><code>label_len</code></td>
<td>Decoder 能看到的“已知答案”长度 (作为提示)</td>
<td><code>dec_inputs</code> 中的 <code>S</code> 及其后续部分</td>
</tr>
<tr>
<td><code>pred_len</code></td>
<td>最终要预测的未来长度</td>
<td><code>target</code> 中除去前缀后的部分</td>
</tr>
</tbody>
</table>
</div>
<p>分完词后，要转换为数字，这也叫Numericalization（数值化），把这些Token映射成词表里的索引数字。例如ich变成1。所以要写一个函数负责数值化，这里起名为<code>make_batch</code>（见后续的代码实现）,然后还要对其进行向量化（Embedding）将数字索引变成浮点数向量比如[0.12, -0.55… ]之类的。pytorch提供了一个方法，在模型里我用nn.Embedding负责向量化。</p>
<p>这样就完成了框架图中的Embedding部分：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260127142731814.png" alt="image-20260127142731814" style="zoom: 25%;"></p>
<h2 id="超参数设置："><a href="#超参数设置：" class="headerlink" title="超参数设置："></a>超参数设置：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span>  <span class="comment"># Embedding Size</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># FeedForward dimension</span></span><br><span class="line">d_k = d_v = <span class="number">64</span>  <span class="comment"># dimension of K(=Q), V</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># number of Encoder of Decoder Layer</span></span><br><span class="line">n_heads = <span class="number">8</span>  <span class="comment"># number of heads in Multi-Head Attention</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数名</th>
<th>大小</th>
<th>深度说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>d_model</code></td>
<td>512</td>
<td>模型维度。它是 Embedding 的长度，也是残差连接（Residual）路径上向量的长度。</td>
</tr>
<tr>
<td><code>d_ff</code></td>
<td>2048</td>
<td>前馈网络中间层维度。在 FFN 中，向量会先从 512 升维到 2048，再降回 512。</td>
</tr>
<tr>
<td><code>d_k = d_q = d_v</code></td>
<td>64</td>
<td>单头维度。每个注意力头在计算时，会将 512 维“切开”进入 64 维的子空间进行观察。</td>
</tr>
<tr>
<td><code>n_layers</code></td>
<td>6</td>
<td>堆叠层数。论文中 Encoder 和 Decoder 分别由 6 个完全相同的层堆叠而成（即 $N=6$）。</td>
</tr>
<tr>
<td><code>n_heads</code></td>
<td>8</td>
<td>头数**。决定了模型同时在多少个不同的语义子空间中进行并行注意力计算。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><p>这个部分实现了注意力公式：<script type="math/tex">\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</script></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k) <span class="comment"># scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>) <span class="comment"># Fills elements of self tensor with value where mask is one.</span></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)</span><br><span class="line">        context = torch.matmul(attn, V)</span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br></pre></td></tr></table></figure>
<ul>
<li>torch.matmul：这个是pytorch的批量矩阵乘法，它会保持前两个维度（batch 和 n_heads）不动，只对最后两个维度执行矩阵乘法。计算的形状变化：<script type="math/tex">[batch, n\_heads, len\_q, d\_k] \times [batch, n\_heads, d\_k, len\_k] = [batch, n\_heads, len\_q, len\_k]</script>表示的意义是对于每一个头，查询句中的每一个词（len_q）对键句中的每一个词（len_k）的关注程度。当然如果是二维矩阵直接用<code>.T</code>也是可以的，对于这种 4 维张量，只想交换最后两个维度还是直接用transpose会更好些</li>
<li>masked_fill_：直接原地修改<code>scores</code>这个变量，将所有之前算出来的attn_mask中维True的位置全部换成一个很小的数字，这样在做softmax之后，$e^{-1e9}$就约等于0</li>
<li>context是$\vec{E}_n + \Delta \vec{E}_n = \vec{E}’_n$中$\Delta \vec{E}_n$的一部分，是每一个头产生的增量。</li>
</ul>
<p>此外关于为什么对最后一个维度做Softmax，是因为公式中是$Q K^T$，所以得到的矩阵行是Q，列是K的相关信息，逐列做归一化，就是在计算每个Token的Q对于句子中每一个Token产生的K的注意力分数的归一化。以那一句德语为例，这句德语：[‘ich’, ‘mochte’, ‘ein’, ‘bier’, ‘P’]由于参数是<code>batch_size = 1, n_heads = 8, d_model = 512, d_k = 64</code>。可以得到矩阵的整体形状是[1, 8, 5, 5]。为了直观，只看其中一个 Head，它的横轴（列）是 Key，纵轴（行）是 Query，可能得注意力分数是：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Query \ Key</th>
<th>ich (1)</th>
<th>mochte (2)</th>
<th>ein (3)</th>
<th>bier (4)</th>
<th>P (0)</th>
<th>行总和</th>
</tr>
</thead>
<tbody>
<tr>
<td>ich</td>
<td>0.8</td>
<td>0.1</td>
<td>0.05</td>
<td>0.05</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr>
<td>mochte</td>
<td>0.2</td>
<td>0.6</td>
<td>0.1</td>
<td>0.1</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr>
<td>ein</td>
<td>0.05</td>
<td>0.1</td>
<td>0.7</td>
<td>0.15</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr>
<td>bier</td>
<td>0.05</td>
<td>0.1</td>
<td>0.25</td>
<td>0.6</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr>
<td>P</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>0.2</td>
<td>1.0</td>
</tr>
</tbody>
</table>
</div>
<p>Softmax 的公式是 $P_i = \frac{e^{x_i}}{\sum e^{x_j}}$,所以最后一行就算全是-1e9那么平均下来每个也能分到0.2。这个注意力分数会影响结果，所以我会在最后计算 Loss 的时候，直接设置ignore_index=0，这样计算 Loss 的时候，在PyTorch 检查 target_batch时，凡是标签值为 0（即字符 ‘P’）的位置，产生的损失都会被直接标记为 0，直接把 P 这个位置产生的任何输出结果给物理抹除。</p>
<p>框架中对应的MultiHeadAttention部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads)</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads)</span><br><span class="line">        self.linear = nn.Linear(n_heads * d_v, d_model)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="comment"># q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]</span></span><br><span class="line">        residual, batch_size = Q, Q.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)</span></span><br><span class="line">        q_s = self.W_Q(Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  <span class="comment"># q_s: [batch_size x n_heads x len_q x d_k]</span></span><br><span class="line">        k_s = self.W_K(K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)  <span class="comment"># k_s: [batch_size x n_heads x len_k x d_k]</span></span><br><span class="line">        v_s = self.W_V(V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>,<span class="number">2</span>)  <span class="comment"># v_s: [batch_size x n_heads x len_k x d_v]</span></span><br><span class="line"></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># attn_mask : [batch_size x n_heads x len_q x len_k]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]</span></span><br><span class="line">        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)</span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, n_heads * d_v) <span class="comment"># context: [batch_size x len_q x n_heads * d_v]</span></span><br><span class="line">        output = self.linear(context)</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(output + residual), attn <span class="comment"># output: [batch_size x len_q x d_model]</span></span><br></pre></td></tr></table></figure>
<p>这个模型在初始化的时候的参数，对应了GPT-3的Query，Key，Value，Output：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260127142911815.png" alt="image-20260127142911815" style="zoom: 50%;"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>对应的层</th>
<th>GPT-3 的参数计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>self.W_Q</code></td>
<td>Query</td>
<td>$d_query \times d_embed \times n_heads \times n_layers$</td>
</tr>
<tr>
<td><code>self.W_K</code></td>
<td>Key</td>
<td>$d_query \times d_embed \times n_heads \times n_layers$</td>
</tr>
<tr>
<td><code>self.W_V</code></td>
<td>Value</td>
<td>$d_value \times d_embed \times n_heads \times n_layers$</td>
</tr>
<tr>
<td><code>self.linear</code></td>
<td>Output</td>
<td>$d_embed \times d_value \times n_heads \times n_layers$</td>
</tr>
</tbody>
</table>
</div>
<p>而代码中的<code>self.layer_norm = nn.LayerNorm(d_model)</code>则是层归一化层，当然他的维度是和词向量大小<code>d_model</code>相关。代码里 n_layers = 6，所以总参数量就是这些线性层参数之和再乘以 6。</p>
<p>q_s、k_s、v_s是对 $Q, K, V$ 做了完全相同的变换，以$Q$为例,</p>
<ul>
<li>第一步是线性投影，对应代码中的<code>self.W_Q(Q)</code>,表示将原始词向量通过矩阵乘法，投影到一个新的特征空间，即$Q_{proj} = Q \cdot W^Q$。</li>
<li>第二步是维度拆分，对应代码中<code>.view(batch_size, -1, n_heads, d_k)</code>表示将原本 $512$ 维的向量，“切”成了 $8$ 个 $64$ 维的小向量。这是多头的物理实现。</li>
<li>第三步是对维度的转置，对应代码中<code>.transpose(1, 2)</code>表示从从 [batch, len, 8, 64] 变成 [batch, 8, len, 64]。这是为了后续的矩阵乘法运算，在刚才的<code>ScaledDotProductAttention</code>中也可以看到是对最后两个维度做运算。</li>
</ul>
<p>而<code>attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)</code>表示把之前做好的单层掩码，克隆成一套多头的掩码矩阵，以便它能准确覆盖到每一个注意力头上。</p>
<ul>
<li>unsqueeze(1)表示在第 1 维（索引从 0 开始）插入一个长度为 1 的新维度。形状变化：[batch_size, len_q, len_k] $\rightarrow$ [batch_size, 1, len_q, len_k]。目的是为注意力头的腾出位置，即在 batch 和序列长度之间插了一个空位。</li>
<li>repeat(1, n_heads, 1, 1)表示在第 1 维拷贝 n_heads 次，其余维不变。形状变化：[batch_size, 1, len_q, len_k] $\rightarrow$ [batch_size, 8, 5, 5]（n_head设置为 8 个头）</li>
</ul>
<p>然后在<code>context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)</code>将 8 个头独立算出的增量重新缝合成一个完整的、与输入维度一致的长向量。</p>
<ul>
<li><code>transpose(1, 2)</code>是在把注意力头换回来，因为计算完注意力后，context 的形状是 [batch_size, n_heads, len_q, d_v]（[1, 8, 5, 64]）。这相当于交换第 1 维（n_heads）和第 2 维（len_q），最后会变成[1, 5, 8, 64]。表示重新以单词序列为主序。现在每个单词后面紧跟着 8 个头为它捕捉到的特征信息。</li>
<li><code>contiguous()</code>是在整理内存，在内存中开辟一块新空间，按照现在的逻辑顺序把数据重新排好。如果不加这一步，下一步的<code>view</code>操作会因为内存不连续而报错。</li>
<li><code>view(batch_size, -1, n_heads * d_v)</code>是在缝合维度，把最后两个维度（8 个头 和 每个头的 64 维）合并成一个维度（$8 \times 64 = 512$）。最终形状：[batch_size, len_q, d_model] (即 [1, 5, 512])。</li>
</ul>
<h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><p>Feed Forward部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PoswiseFeedForwardNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        residual = inputs <span class="comment"># inputs : [batch_size, len_q, d_model]</span></span><br><span class="line">        output = nn.ReLU()(self.conv1(inputs.transpose(<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">        output = self.conv2(output).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(output + residual)</span><br></pre></td></tr></table></figure>
<p>这一部分也就是多层感知机。</p>
<p>在init中：</p>
<ul>
<li><code>self.conv1</code> 表示将 d_model (512) 映射到 d_ff (2048)。即特征放大，寻找更细微的模式。对应图中第一个线性层</li>
<li><code>self.conv2</code>表示将 d_ff (2048) 压缩回 d_model (512)。用于还原原始的词向量</li>
<li><code>self.layer_norm</code>层归一化，这图中没有标明，实际位置在$\oplus$ 之后，这个实际上是在将得到的新词向量做一个归一化，虽然在进入 <code>PoswiseFeedForwardNet</code> 时，多头的 <code>context</code> 早就已经被缝合成一个统一的向量了,主要是为了防止向量里的数值在经过 $512 \rightarrow 2048 \rightarrow 512$ 的剧烈变换后变得忽大忽小。确保下一层（比如另一个 Encoder 层）接收到的输入是标准的，不会导致梯度爆炸。</li>
</ul>
<p>对应了GPT-3参数图中的up-projection和down-projection：</p>
<h3 id="Encoder相关"><a href="#Encoder相关" class="headerlink" title="Encoder相关"></a>Encoder相关</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    <span class="comment"># 比如在Encoder的调用里面，都是 enc_inputs: [[1, 2, 3, 4, 0]]</span></span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># eq(0) 是判断哪些位置等于 0 (即 P)</span></span><br><span class="line">    <span class="comment"># pad_attn_mask 变成: [[False, False, False, False, True]]</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># batch_size x 1 x len_k(=len_q), one is masking</span></span><br><span class="line">    <span class="comment"># expand 扩展成矩阵: [1, 5, 5]</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  <span class="comment"># batch_size x len_q x len_k</span></span><br></pre></td></tr></table></figure>
<p>函数返回的是一个 布尔矩阵（Mask 矩阵）。对于 ich mochte ein bier P 这句话，它生成的 Mask 矩阵看起来像这样（用 1 代表 True/屏蔽）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>ich</th>
<th>mochte</th>
<th>ein</th>
<th>bier</th>
<th>P</th>
</tr>
</thead>
<tbody>
<tr>
<td>ich</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>mochte</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>…</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>P</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>表示无论这个Token是谁（每一行代表一个词），当这个Token回头看全句时，最后一列的 P 都要被抹掉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_sinusoid_encoding_table</span>(<span class="params">n_position, d_model</span>):</span><br><span class="line">    <span class="comment"># 算角度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cal_angle</span>(<span class="params">position, hid_idx</span>):</span><br><span class="line">        <span class="keyword">return</span> position / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_idx // <span class="number">2</span>) / d_model)</span><br><span class="line">    <span class="comment"># 算向量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_posi_angle_vec</span>(<span class="params">position</span>):</span><br><span class="line">        <span class="keyword">return</span> [cal_angle(position, hid_j) <span class="keyword">for</span> hid_j <span class="keyword">in</span> <span class="built_in">range</span>(d_model)]</span><br><span class="line">    <span class="comment"># 先求出公式中括号里面的</span></span><br><span class="line">    sinusoid_table = np.array([get_posi_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> <span class="built_in">range</span>(n_position)])</span><br><span class="line">    <span class="comment"># 分给sin和cos </span></span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line">    sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line">    <span class="keyword">return</span> torch.FloatTensor(sinusoid_table)</span><br></pre></td></tr></table></figure>
<p>代码中的逻辑其实是在实现论文里的这两个公式：</p>
<script type="math/tex; mode=display">
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)</script><script type="math/tex; mode=display">
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)</script><p>三个原因：</p>
<p>第一个是有界性的考虑。$\sin$ 和 $\cos$ 的值永远在 $[-1, 1]$ 之间。如果直接用 $1, 2, 3…$，当句子很长时，位置值会变得巨大，干扰词向量的特征。</p>
<p>第二个是相对位置关系方便表示，根据三角函数公式：</p>
<script type="math/tex; mode=display">\sin(\alpha + \beta) = \sin\alpha\cos\beta + \cos\alpha\sin\beta</script><p>这说明对于任何固定的偏移 $k$，$PE_{pos+k}$ 都可以表示为 $PE_{pos}$ 的线性组合。这让模型能够非常容易地学习到词与词之间的相对距离。</p>
<p>第三就是唯一性。由于每一维的频率（分母里的 $10000^{…}$）都不同，从第 0 维到第 511 维，波长是逐渐拉长的。这保证了每一个位置生成的 512 维向量都是唯一的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__() <span class="comment"># 初始化父类，确保能够使用nn.Module中的自动追踪权重、支持移动到GPU等</span></span><br><span class="line">        self.enc_self_attn = MultiHeadAttention() <span class="comment"># 多头注意力</span></span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet() <span class="comment"># 前馈神经层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) <span class="comment"># 去做自注意力，Q,K,V都是enc_inputs</span></span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs) <span class="comment"># enc_outputs: [batch_size x len_q x d_model]</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br></pre></td></tr></table></figure>
<p>enc_outputs是经过多头注意力（Multi-Head Attention）和前馈网络（FFN）处理后的新向量。</p>
<ul>
<li>进去前向量只代表词本身（比如 bier 只代表啤酒）。</li>
<li>出来后向量融入了周围词的信息（比如 bier 的向量现在包含了“它是 ich 想要的那个 bier”这种语境信息）。</li>
</ul>
<p>这个 enc_outputs 会作为下一层 Encoder Layer 的输入继续被精炼。层数给了 6 层，相当于被精炼 6 次。</p>
<p>attn是注意力权重矩阵（打分表）就是产生了注意力分数的矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model) <span class="comment">#Token的Embedding</span></span><br><span class="line">        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+<span class="number">1</span>, d_model),freeze=<span class="literal">True</span>) <span class="comment">#位置编码</span></span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]) <span class="comment">#产生n_layers个层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>): <span class="comment"># enc_inputs : [batch_size x source_len]</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">0</span>]])) <span class="comment">#特征融合，整合词义信息和位置信息</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) <span class="comment"># 在计算注意力时，忽略为了凑长度而补进去的 P（Padding）。</span></span><br><span class="line">        enc_self_attns = [] <span class="comment">#存每一层的注意力分数表，可视化会用到</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure>
<ul>
<li>nn.Embedding：nn.Embedding就是一个模型能够理解的字典。第一维是词表大小，是整个字典里有多少个不重复的词，因为是Encoder层，所以词表大小是src_vocab_size。d_model是每一个 Token 被转化成的向量维度（这里是 512）。当然，在更高级的 NLP 任务中，经常会加载预训练词向量而不是让模型自己去学（如 Word2Vec 或 GloVe）。</li>
<li>Embedding.from_pretrained：from_pretrained 是一个特殊的构造方法。它改变了字典的生成方式。普通构造是需要去训练的，构造出来的全是全是随机生成的数字，但是这个方法会给一个已经算好的矩阵（Tensor）让它直接用这些数字作为初始内容。具体的数字源于get_sinusoid_encoding_table 函数，这个函数通过 $sin$ 和 $cos$ 的数学公式，预先计算出了一个形状为 [6, 512] 的矩阵。这里多加了一维，为了兼容Decoder中的‘S’。</li>
<li>特征融合这里，self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))这个顺序是和输入的句子意义对应的：’ich mochte ein bier P’。当然这是为了演示，真实情况肯定不是这样硬编码的</li>
<li>for循环是在精炼语义，可能得情况是第 1 层理解基础语法，比如主谓关系。到第 6 层就能够理解高级语义，即词语在句子里的最终含义。最后返回的 enc_outputs就是 Encoder 对这句德语的最终理解结果。它会被送往 Decoder，作为翻译的参考。</li>
</ul>
<p>这样就完成了模型框架中的词嵌入以及编码层。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260127143142655.png" alt="image-20260127143142655" style="zoom: 25%;"></p>
<h3 id="Dncoder相关"><a href="#Dncoder相关" class="headerlink" title="Dncoder相关"></a>Dncoder相关</h3><p><code>get_attn_subsequent_mask</code>是和Masked Multi-Head Attention相关，Encoder 的 Mask 只是为了挡住 P (Padding)，而这个方法生成的是上三角矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequent_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)</span><br><span class="line">    subsequent_mask = torch.from_numpy(subsequent_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试，模拟一个 Batch_size=1, 长度为 5 的序列</span></span><br><span class="line">test_seq = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">mask = get_attn_subsequent_mask(test_seq)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Generated Subsequent Mask (1 means MASKED):&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(mask[<span class="number">0</span>].numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Generated Subsequent Mask (1 means MASKED):
[[0 1 1 1 1]
 [0 0 1 1 1]
 [0 0 0 1 1]
 [0 0 0 0 1]
 [0 0 0 0 0]]
</code></pre><p>[[0 1 1 1 1]  第一行：只能看索引0，不能看1,2,3,4</p>
<p> [0 0 1 1 1]  第二行：只能看索引0,1，不能看2,3,4</p>
<p> [0 0 0 1 1]  第三行：只能看索引0,1,2，不能看3,4</p>
<p> [0 0 0 0 1]  第四行：只能看索引0,1,2,3，不能看4</p>
<p> [0 0 0 0 0]] 第五行：全都能看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn</span><br></pre></td></tr></table></figure>
<ul>
<li><code>self.dec_self_attn</code>：这个是先做了一个自我回顾，$Q, K, V$ 全部来自 <code>dec_inputs</code>（即目标语言序列 “S I want a beer”）。并且使用的是<code>dec_self_attn_mask</code>这个上三角矩阵。这是在让模型在产生当前单词时，能够参考已经生成出来的单词。因为有了上三角掩码，模型在这一步只能看到历史信息，不能偷看后面的结果。</li>
<li><code>self.dec_enc_attn</code>：这一步是在做交叉注意力，上一步产生的<code>dec_outputs</code>作为$Q$ (Query)，<code>enc_outputs</code>作为$K$ (Key) 和 $V$ (Value)，代表通过Encoder后原文（德语）的所有特征信息。注意这里的<code>dec_enc_attn_mask</code>掩码只负责遮住原文里的 P (Padding)，不需要遮住未来，因为之后的句子本身就看不到。这里就是真的在做翻译了，模型拿着已经翻译出的英语单词作为搜索词，去德语原文里寻找最相关的词（比如翻译 “beer” 时，它会去原文里定位到 “bier”）。</li>
<li><code>self.pos_ffn</code>：这个是做了一个全连接，将整合的结果进入到前馈神经网络（MLP）来对融合后的特征进行非线性映射和进一步升维提取</li>
</ul>
<p>Encoder层和Decoder层对比：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特性</th>
<th>EncoderLayer</th>
<th>DecoderLayer</th>
</tr>
</thead>
<tbody>
<tr>
<td>层数</td>
<td>2 层 (Self-Attn + FFN)</td>
<td>3 层 (Self-Attn + Cross-Attn + FFN)</td>
</tr>
<tr>
<td>掩码</td>
<td>只遮 Padding</td>
<td>Self-Attn 遮未来的词汇；Cross-Attn 遮原文的 Padding</td>
</tr>
<tr>
<td>数据源</td>
<td>纯粹的源语言</td>
<td>混合源语言（K, V）和目标语言（Q）</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+<span class="number">1</span>, d_model),freeze=<span class="literal">True</span>)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>): <span class="comment"># dec_inputs : [batch_size x target_len]</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]]))</span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)</span><br><span class="line">        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)</span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)</span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<p>这个和Encoder类很像，它把多个 DecoderLayer 堆叠起来，并初始化目标词向量。</p>
<ul>
<li><code>torch.gt</code>：gt 是 “Greater Than” 的缩写，先将两张矩阵相加,相加后，只要某个位置原本在任意一张矩阵里是 1，加完后的值就会是 1 或 2,也就大于0，并将其转换为布尔值。大于0的是True，小于0是False。这样就能整合pad的掩码以及遮掉上三角矩阵的掩码，这就保证了模型在自注意力阶段既不会看 P，也不会看未来的词。</li>
<li><code>dec_enc_attn_mask</code>：遮住的是 <code>enc_inputs</code> 里的 P，为了 <code>DecoderLayer</code> 里的第二层注意力准备的。</li>
<li>for循环同样是在精炼语义，最后整合Decoder 对这句德语的最终理解结果。</li>
</ul>
<h3 id="Transformer主类"><a href="#Transformer主类" class="headerlink" title="Transformer主类"></a>Transformer主类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder()</span><br><span class="line">        self.decoder = Decoder()</span><br><span class="line">        <span class="comment"># 最后的线性投影层：将向量打回词表大小</span></span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="comment"># 第一步：原文进，输出特征</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment"># 第二步：译文前缀+原文特征进入，生成特征</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        <span class="comment"># 第三步：将 [batch, len, 512] 映射为 [batch, len, 7]</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs) <span class="comment"># dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]</span></span><br><span class="line">        <span class="comment"># 第四步：变形为 [batch*len, 7] 方便计算交叉熵损失，view是在缝合前两个维度</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<h2 id="画图函数"><a href="#画图函数" class="headerlink" title="画图函数"></a>画图函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">showgraph</span>(<span class="params">attn,name</span>):</span><br><span class="line">    <span class="comment"># 传入一个注意力矩阵，当然一个三个维度，第一个维度表示是哪一个注意力头，这里选取 [-1] 代表取最后一层的输出，因为最后一层的理解通常最深刻。</span></span><br><span class="line">    attn = attn[-<span class="number">1</span>].squeeze(<span class="number">0</span>)[<span class="number">0</span>] <span class="comment"># squeeze(0)：去掉 Batch 维度（即从 [1, 8, 5, 5] 变成 [8, 5, 5]）。并且取了第一个头的注意力分数</span></span><br><span class="line">    attn = attn.data.numpy()</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(figsize=(n_heads, n_heads))</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    im = ax.matshow(attn, cmap=<span class="string">&#x27;viridis&#x27;</span>) <span class="comment"># 把矩阵里的权重数值（0 到 1 之间）映射成颜色（紫色到黄色）。颜色越亮（趋向黄色），代表注意力权重越高。</span></span><br><span class="line"></span><br><span class="line">    ax.set_xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(sentences[<span class="number">0</span>].split())))</span><br><span class="line">    ax.set_yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(sentences[<span class="number">2</span>].split())))</span><br><span class="line">    <span class="comment"># 坐标轴的数字索引换成真正的单词</span></span><br><span class="line">    ax.set_xticklabels(sentences[<span class="number">0</span>].split(), rotation=<span class="number">90</span>)</span><br><span class="line">    ax.set_yticklabels(sentences[<span class="number">2</span>].split())</span><br><span class="line"></span><br><span class="line">    plt.colorbar(im)</span><br><span class="line">    plt.savefig(name+<span class="string">&#x27;attention.png&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="模型训练与结果可视化"><a href="#模型训练与结果可视化" class="headerlink" title="模型训练与结果可视化"></a>模型训练与结果可视化</h2><p>随机种子函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed=<span class="number">2026</span></span>):</span><br><span class="line">    <span class="keyword">import</span> random</span><br><span class="line">    random.seed(seed)                          <span class="comment"># 1. 锁定 Python 原生随机数</span></span><br><span class="line">    np.random.seed(seed)                       <span class="comment"># 2. 锁定 NumPy 随机数</span></span><br><span class="line">    torch.manual_seed(seed)                    <span class="comment"># 3. 锁定 CPU 上的 PyTorch</span></span><br><span class="line">    torch.cuda.manual_seed(seed)               <span class="comment"># 4. 锁定 GPU 上的 PyTorch</span></span><br><span class="line">    torch.cuda.manual_seed_all(seed)           <span class="comment"># 5. 如果使用多块 GPU，全锁定</span></span><br><span class="line">    <span class="comment"># 6. 确定性运算（会稍微降低一点点训练速度，但能保证结果一模一样）</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">set_seed(<span class="number">2026</span>) <span class="comment"># 调用函数，设置一个数字</span></span><br></pre></td></tr></table></figure>
<p>初始化模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Transformer()</span><br></pre></td></tr></table></figure>
<p>在这之前要先讲sentences进行数值化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_batch</span>(<span class="params">sentences</span>):</span><br><span class="line">    input_batch = [[src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[<span class="number">0</span>].split()]]</span><br><span class="line">    output_batch = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[<span class="number">1</span>].split()]]</span><br><span class="line">    target_batch = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[<span class="number">2</span>].split()]]</span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)</span><br></pre></td></tr></table></figure>
<p>交叉熵作为损失函数，Adam作为优化器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>) <span class="comment"># 调小一点</span></span><br><span class="line">enc_inputs, dec_inputs, target_batch = make_batch(sentences) <span class="comment"># 数值化原始句子</span></span><br><span class="line">enc_inputs, dec_inputs, target_batch</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<pre><code>(tensor([[1, 2, 3, 4, 0]]),
 tensor([[5, 1, 2, 3, 4]]),
 tensor([[1, 2, 3, 4, 6]]))
</code></pre><p>模型的训练部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 把上限设高一点，交给早停来控制</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)</span><br><span class="line">    loss = criterion(outputs, target_batch.contiguous().view(-<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打印进度</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># --- 早停 ---</span></span><br><span class="line">    <span class="keyword">if</span> loss &lt; <span class="number">0.0001</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;已达到预期精度，在第 %d 轮提前停止训练&#x27;</span> % (epoch + <span class="number">1</span>),<span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>多训练几轮，反正就一句话，设置loss为0.0001就停，结果：</p>
<pre><code>Epoch: 0010 cost = 0.001163
Epoch: 0020 cost = 0.000583
Epoch: 0030 cost = 0.000113
已达到预期精度，在第 31 轮提前停止训练 cost = 0.000100
</code></pre><p>测试部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test</span></span><br><span class="line">predict, _, _, _ = model(enc_inputs, dec_inputs)</span><br><span class="line">predict = predict.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(sentences[<span class="number">0</span>], <span class="string">&#x27;-&gt;&#x27;</span>, [number_dict[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;first head of last state enc_self_attns&#x27;</span>)</span><br><span class="line">showgraph(enc_self_attns,<span class="string">&#x27;enc_self_attns&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;first head of last state dec_self_attns&#x27;</span>)</span><br><span class="line">showgraph(dec_self_attns,<span class="string">&#x27;dec_self_attns&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;first head of last state dec_enc_attns&#x27;</span>)</span><br><span class="line">showgraph(dec_enc_attns,<span class="string">&#x27;dec_enc_attns&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>结果</p>
<pre><code>ich mochte ein bier P -&gt; [&#39;i&#39;, &#39;want&#39;, &#39;a&#39;, &#39;beer&#39;, &#39;E&#39;]
first head of last state enc_self_attns
</code></pre><p>这会跑出来3个图。</p>
<p>第一张图：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260127143601965.png" alt="image-20260127143601965" style="zoom: 50%;"></p>
<p>Encoder 自注意力图。展示的是德语单词之间的自我关联（自注意）情况。每个词在关注自己的同时，也对整句的其他词有均匀的亮度分布。最后的 P（Padding）列是黑色的，说明 get_attn_pad_mask 成功工作了，模型忽略了无意义的填充符。</p>
<p>第二张图：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260127143622426.png" alt="image-20260127143622426" style="zoom:50%;"></p>
<p>Decoder 自注意力图。右上角完全黑暗的部分说明了代码中<code>get_attn_subsequent_mask</code>得到的上三角矩阵起作用了。</p>
<p>第三张图：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260127143633214.png" alt="image-20260127143633214" style="zoom:50%;"></p>
<p>Decoder-Encoder 跨注意力图。有对角线对齐趋势，这说明成功地在德语原文中定位到了对应的词汇。</p>
<h1 id="Transformer4TS"><a href="#Transformer4TS" class="headerlink" title="Transformer4TS"></a>Transformer4TS</h1><p><a target="_blank" rel="noopener" href="https://github.com/thuml/Time-Series-Library/tree/main">thuml/Time-Series-Library: A Library for Advanced Deep Time Series Models for General Time Series Analysis.</a></p>
<h2 id="启动脚本的超参数解读"><a href="#启动脚本的超参数解读" class="headerlink" title="启动脚本的超参数解读"></a>启动脚本的超参数解读</h2><p>之前涉及到的概念（$Q, K, V$, 多头注意、Embedding、参数量），在run.py里都有对应的超参数。</p>
<ul>
<li><code>--d_model</code> :<ul>
<li>这就是一直在说的词向量维度 $d$。</li>
<li>它决定了每一个 Token 向量有多长。 $W_Q, W_K, W_V$ 矩阵的形状，就是由这个数字决定的。</li>
</ul>
</li>
<li><p><code>--e_layers</code> / <code>--d_layers</code> :</p>
<ul>
<li>Encoder（编码器）和 Decoder（解码器）的层数。即信息交换与模型推理的地方。</li>
<li>GPT-3 有 96 层，且这96层全是解码器的层数。</li>
</ul>
</li>
<li><p><code>--n_heads</code> :</p>
<ul>
<li>多头注意力（Multi-head Attention）。</li>
<li>这代表模型会同时开启 4 个头并行计算。每个头都有自己独立的 $W_Q, W_K, W_V$，分别去寻找不同的规律（比如头1找周期性，头2找趋势）。</li>
</ul>
</li>
<li><p><code>--d_ff</code>:</p>
<ul>
<li>前馈神经网络（Feed Forward）的维度。</li>
<li>这就是Up-projection。在注意力交换完情报后，向量会进入这个2048维的空间进行非线性变换。</li>
</ul>
</li>
<li><p><code>--enc_in</code> :</p>
<ul>
<li>输入特征数。</li>
<li>这代表你的初始元素，对应了嵌入前的初始文本。比如有 7 种时间序列数据，Embedding 层会负责把这 7 个维度变成 <code>d_model</code>。</li>
</ul>
</li>
<li><p><code>--embed</code> :</p>
<ul>
<li>对应Embedding 的方式。</li>
<li>这决定了模型如何把“时间”这个抽象概念编码进向量里。</li>
</ul>
</li>
<li><p><code>--dropout</code> (0.1): 在加权求和时，随机丢弃 10% 的神经元，防止模型过拟合。</p>
</li>
<li><code>--activation</code> (‘gelu’): 这是激活函数。在 $V$ 向量搬运完信息后，给“思考”过程增加一点非线性的逻辑，GPT 系列通常都是GELU。</li>
</ul>
<p>时间序列和文本任务在 Transformer 的应用上，存在几个核心的策略差异，文本任务（如 GPT）是目的是压缩，他文本原始维度是词表大小（5万+），d_model给的是1.2万，做特征降维和语义聚合。</p>
<p>但是时间序列，原始维度一般很小，达不到$10^4$级别，我做的数据集中最多也才26维。直接对 26 个维度做注意力计算，信息量太薄，模型很难学到复杂的非线性关系。因此一般会通过 Embedding 层将 26 维投影到 64 维。在时间序列论文（如 TimesNet, Informer, Autoformer）中，通常 d_model 设置为原始特征数的 2 到 4 倍 是比较稳妥的起点。所以在试的时候 d_model 从默认的 512 改成 64,48,32比较合适。当然这样做模型中所有 $W_Q, W_K, W_V$ 矩阵的面积缩小不少，跑的还是挺快的。</p>
<p>公式中$\sqrt{d_k}$ 的计算：在 <code>d_model=64</code>, <code>n_heads=4</code> 的情况下，每一个“头”分到的向量维度 $d_k$ 是16个，因为$d_k$ 代表Key向量的维度。那么由<code>d_model=64</code>得每一个头分到的是64/4=16个。</p>
<h2 id="Transformer模型代码解读"><a href="#Transformer模型代码解读" class="headerlink" title="Transformer模型代码解读"></a>Transformer模型代码解读</h2><p>时间序列领域的 Transformer和原先的NLP任务的Transformer会有所不同。虽然它的底层还是 Transformer，但为了适应连续数值而非离散单词需要做一些改变。</p>
<p>在这个时间序列库中，Transformer的模型代码，在预测任务（forecast）中是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forecast</span>(<span class="params">self, x_enc, x_mark_enc, x_dec, x_mark_dec</span>):</span><br><span class="line">    <span class="comment"># Embedding</span></span><br><span class="line">    enc_out = self.enc_embedding(x_enc, x_mark_enc)</span><br><span class="line">    enc_out, attns = self.encoder(enc_out, attn_mask=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    dec_out = self.dec_embedding(x_dec, x_mark_dec)</span><br><span class="line">    dec_out = self.decoder(dec_out, enc_out, x_mask=<span class="literal">None</span>, cross_mask=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">return</span> dec_out</span><br></pre></td></tr></table></figure>
<p>主要是嵌入方式和输入结构的改变。</p>
<p>另外，时间序列用的是滑动窗口构造数据，比如一条10,000 个点的温度曲线，不会一次性把 10,000 个点塞给 Transformer。会用一个比如长度为 96 的窗口，从头滑到尾。这样你就得到了很多个Segments。</p>
<h3 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h3><p>和原先的NLP不同，原始的只有Token和Position信息，他还包括了一个时间信息（Temporal Embedding）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>对应代码类</strong></th>
<th><strong>解决的问题</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>数值特征</strong></td>
<td><code>TokenEmbedding</code></td>
<td>现在的XX数值是多少</td>
</tr>
<tr>
<td><strong>周期特征</strong></td>
<td><code>TemporalEmbedding</code></td>
<td>现在是几点/星期几</td>
</tr>
<tr>
<td><strong>位置特征</strong></td>
<td><code>PositionalEmbedding</code></td>
<td>这是序列里的第几个点</td>
</tr>
</tbody>
</table>
</div>
<p>通过相加的方式，把数值是多少、当时是几点、在序列第几个点这三个维度的信息压缩进了同一个向量$x$里：</p>
<script type="math/tex; mode=display">
x = ValueEmbedding(x) + TemporalEmbedding(x\_mark) + PositionalEmbedding(x)</script><p>self.enc_embedding的初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.enc_embedding = DataEmbedding(</span><br><span class="line">    configs.enc_in,   <span class="comment"># 1. 变量数（比如有几个指标：温度、湿度、气压...）</span></span><br><span class="line">    configs.d_model,  <span class="comment"># 2. 隐藏层维度（把指标变成多长的向量，如 512）</span></span><br><span class="line">    configs.embed,    <span class="comment"># 3. 时间嵌入类型（&#x27;fixed&#x27; 用正弦，&#x27;learned&#x27; 用参数）</span></span><br><span class="line">    configs.freq,     <span class="comment"># 4. 时间频率（&#x27;h&#x27;:小时, &#x27;t&#x27;:分钟, &#x27;s&#x27;:秒... 决定了时间表的颗粒度）</span></span><br><span class="line">    configs.dropout   <span class="comment"># 5. 随机丢弃率（防止过拟合）</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这个<code>DataEmbedding</code>是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c_in, d_model, embed_type=<span class="string">&#x27;fixed&#x27;</span>, freq=<span class="string">&#x27;h&#x27;</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DataEmbedding, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)</span><br><span class="line">        self.position_embedding = PositionalEmbedding(d_model=d_model)</span><br><span class="line">        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,freq=freq) <span class="keyword">if</span> embed_type != <span class="string">&#x27;timeF&#x27;</span> <span class="keyword">else</span> TimeFeatureEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, x_mark</span>):</span><br><span class="line">        <span class="keyword">if</span> x_mark <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.value_embedding(x) + self.position_embedding(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 将数值位置周期嵌入信息相加</span></span><br><span class="line">            x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>这里是直接将所有的信息相加，结果依然是 <code>[Batch, Length, 512]</code>。这不会弄乱，假设输入向量 $x$ 经过一个线性层 $W$：</p>
<ul>
<li>拼接后运算：$[v; p] \times [W_1; W_2] = vW_1 + pW_2$</li>
<li>相加后运算：$(v + p) \times W = vW + pW$</li>
</ul>
<p>所以其实如果模型学习出的 $W$ 内部能够区分出处理 $v$ 的部分和处理 $p$ 的部分，相加的效果在数学表达力上就不会比拼接差，但它极大地节省了内存。，相加确实会造成一定的干扰，但因为512相对输入层的维度很高，之前也说了这些特征分布很稀疏，理想情况下能够达到正交，所以模型还是可以学到这些信息。而且就算拼接看上去模型要学的东西少了，实际上相加能让梯度传导更直接，也在训练的时候降低了学习的时间成本。</p>
<p>先看<code>self.value_embedding</code>中的<code>TokenEmbedding</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TokenEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c_in, d_model</span>):</span><br><span class="line">        <span class="built_in">super</span>(TokenEmbedding, self).__init__()</span><br><span class="line">        padding = <span class="number">1</span> <span class="keyword">if</span> torch.__version__ &gt;= <span class="string">&#x27;1.5.0&#x27;</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">        <span class="comment"># 这里参数是circular，这个填充的好处是循环填充，即把序列开头的部分补到结尾，结尾补到开头。因为时间序列通常是有周期的（比如每天 24 小时循环）。这样填充这有助于捕捉季节性（Seasonality）特征。</span></span><br><span class="line">        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,kernel_size=<span class="number">3</span>, padding=padding, padding_mode=<span class="string">&#x27;circular&#x27;</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv1d):</span><br><span class="line">                <span class="comment">#kaiming_normal_这个就是一个专门为深层网络设计的科学初始化方案。</span></span><br><span class="line">                <span class="comment">#m.weight 里的 weight（权重）是 nn.Conv1d 类内部自带的一个属性，</span></span><br><span class="line">                <span class="comment">#对于这份代码，这个 weight 矩阵的形状是： [d_model, c_in, 3] (输出通道数, 输入通道数, 卷积核大小)</span></span><br><span class="line">                <span class="comment">#Fan-in 指的是输入单元的数量（即d_model个神经元连过来），mode=&#x27;fan_in&#x27;会根据输入维度的大小来缩放权重</span></span><br><span class="line">                <span class="comment">#这里的激活函数选择了&#x27;leaky_relu&#x27;Leaky ReLU 会给负数保留一个小小的斜率，和GELU差不多</span></span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="string">&#x27;leaky_relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x.permute(0, 2, 1)的原因是PyTorch 的 Conv1d 规定，卷积核必须在最后一个维度上滑动。</span></span><br><span class="line">        <span class="comment">#所以要把时间长度（Length）挪到了最后。</span></span><br><span class="line">        <span class="comment">#然后transpose把数据变回 [Batch, Length, d_model]，这样后面的 Transformer 层才能认得出它。</span></span><br><span class="line">        x = self.tokenConv(x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>他和传统的NLP Transformer不一样的是，这个Token的嵌入用的是卷积来提取局部特征，代码里<code>kernel_size=3</code>那么就是对这个时间片段(Segment)的[第1, 2, 3] 个点的的数值计算出一个综合特征（比如这三个点是上升的还是下降的）。然后移动一格，在 [第2, 3, 4] 个点上，算出下一个特征。如果只用普通的线性投影（Linear），模型一次只能看到一个点。而卷积让模型在第一步（Embedding）时，就能通过这个“窗口”捕捉到局部趋势（局部斜率、波动幅度等）。</p>
<p>此外，还要对时间进行嵌入，因为在数字上，23 点和 0 点差了 23，但在时间逻辑上，23 点（深夜）和 0 点（凌晨）是非常接近的。<code>TemporalEmbedding</code>是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TemporalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, embed_type=<span class="string">&#x27;fixed&#x27;</span>, freq=<span class="string">&#x27;h&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TemporalEmbedding, self).__init__()</span><br><span class="line">		<span class="comment"># 这里分钟设置为4不是60一个周期是因为因为在很多公开数据集（如 ETT 数据集）中，</span></span><br><span class="line">        <span class="comment">#数据是每 15 分钟采集一次的（0, 15, 30, 45 分钟），所以对应的索引只有 0, 1, 2, 3 这四个。</span></span><br><span class="line">        minute_size = <span class="number">4</span> </span><br><span class="line">        hour_size = <span class="number">24</span></span><br><span class="line">        weekday_size = <span class="number">7</span></span><br><span class="line">        day_size = <span class="number">32</span> <span class="comment">#多设一个通常是为了兼容Padding</span></span><br><span class="line">        month_size = <span class="number">13</span> <span class="comment">#多设一个通常是为了兼容Padding</span></span><br><span class="line">		</span><br><span class="line">        <span class="comment"># Embed 的选择：</span></span><br><span class="line">        <span class="comment"># nn.Embedding是让模型自己在训练过程中学习“周一”和“周五”的区别</span></span><br><span class="line">        <span class="comment"># FixedEmbedding是正弦/余弦编码，</span></span><br><span class="line">        <span class="comment"># 它不需要学习，而是通过数学公式强制规定时间点之间的联系（比如 11 点和 12 点在数学上就是更接近的）。</span></span><br><span class="line">        Embed = FixedEmbedding <span class="keyword">if</span> embed_type == <span class="string">&#x27;fixed&#x27;</span> <span class="keyword">else</span> nn.Embedding</span><br><span class="line">        <span class="keyword">if</span> freq == <span class="string">&#x27;t&#x27;</span>:</span><br><span class="line">            self.minute_embed = Embed(minute_size, d_model)</span><br><span class="line">        self.hour_embed = Embed(hour_size, d_model)</span><br><span class="line">        self.weekday_embed = Embed(weekday_size, d_model)</span><br><span class="line">        self.day_embed = Embed(day_size, d_model)</span><br><span class="line">        self.month_embed = Embed(month_size, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.long()</span><br><span class="line">        <span class="comment"># hasattr的作用：如果数据集是按“小时”采集的（没有分钟信息），</span></span><br><span class="line">        <span class="comment"># 模型在初始化时就不会创建 minute_embed，前向传播时会自动用 0 代替，而不会报错。</span></span><br><span class="line">        minute_x = self.minute_embed(x[:, :, <span class="number">4</span>]) <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;minute_embed&#x27;</span>) <span class="keyword">else</span> <span class="number">0.</span></span><br><span class="line">        hour_x = self.hour_embed(x[:, :, <span class="number">3</span>])</span><br><span class="line">        weekday_x = self.weekday_embed(x[:, :, <span class="number">2</span>])</span><br><span class="line">        day_x = self.day_embed(x[:, :, <span class="number">1</span>])</span><br><span class="line">        month_x = self.month_embed(x[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hour_x + weekday_x + day_x + month_x + minute_x</span><br></pre></td></tr></table></figure>
<p>至于为什么minute_x取的是第四列，hour_x取了第三列……，这个是和data_loader中写的类有关联，每一个数据集的处理方式不同，比如我有一个数据集是第一列是这个格式：<code>2024/1/2  0:00:00</code>那么我要能匹配上，我应该这样写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过 pd.to_datetime 把字符串转成了真正的时间对象，这个对象自带.month等属性。</span></span><br><span class="line">df_stamp[<span class="string">&#x27;date&#x27;</span>] = pd.to_datetime(df_stamp.date)</span><br><span class="line"><span class="comment"># apply：对这一列里的每一行数据，都执行一遍括号里的操作，1 对应的是 axis=1（按行处理）</span></span><br><span class="line">df_stamp[<span class="string">&#x27;month&#x27;</span>] = df_stamp.date.apply(<span class="keyword">lambda</span> row: row.month, <span class="number">1</span>)</span><br><span class="line">df_stamp[<span class="string">&#x27;day&#x27;</span>] = df_stamp.date.apply(<span class="keyword">lambda</span> row: row.day, <span class="number">1</span>)</span><br><span class="line">df_stamp[<span class="string">&#x27;weekday&#x27;</span>] = df_stamp.date.apply(<span class="keyword">lambda</span> row: row.weekday(), <span class="number">1</span>)</span><br><span class="line">df_stamp[<span class="string">&#x27;hour&#x27;</span>] = df_stamp.date.apply(<span class="keyword">lambda</span> row: row.hour, <span class="number">1</span>)</span><br><span class="line">data_stamp = df_stamp.drop([<span class="string">&#x27;date&#x27;</span>], <span class="number">1</span>).values</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>data_stamp 索引</th>
<th>代码生成的列</th>
<th><code>TemporalEmbedding</code> 抓取的索引</th>
<th>对应的层</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><code>month</code></td>
<td><code>x[:, :, 0]</code></td>
<td><code>self.month_embed</code></td>
</tr>
<tr>
<td>1</td>
<td><code>day</code></td>
<td><code>x[:, :, 1]</code></td>
<td><code>self.day_embed</code></td>
</tr>
<tr>
<td>2</td>
<td><code>weekday</code></td>
<td><code>x[:, :, 2]</code></td>
<td><code>self.weekday_embed</code></td>
</tr>
<tr>
<td>3</td>
<td><code>hour</code></td>
<td><code>x[:, :, 3]</code></td>
<td><code>self.hour_embed</code></td>
</tr>
</tbody>
</table>
</div>
<p>另外里面有一个<code>FixedEmbedding</code>这个是规定了时间的周期规律，代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c_in, d_model</span>):</span><br><span class="line">        <span class="built_in">super</span>(FixedEmbedding, self).__init__()</span><br><span class="line">        <span class="comment"># 创建一个全零矩阵 [最大容量, 维度]，用于存放正弦余弦值</span></span><br><span class="line">        w = torch.zeros(c_in, d_model).<span class="built_in">float</span>()</span><br><span class="line">        w.require_grad = <span class="literal">False</span> <span class="comment"># # 显式声明了这部分不需要计算梯度（不可训练）</span></span><br><span class="line">		<span class="comment"># 生成一个位置序列 [0, 1, 2, ..., c_in-1]，并增加一个维度</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, c_in).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 计算衰减因子（频率项）, Transformer 论文里的标准公式。</span></span><br><span class="line">        div_term = (torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">                    * -(math.log(<span class="number">10000.0</span>) / d_model)).exp()</span><br><span class="line">		<span class="comment"># 偶数列填 sin，奇数列填 cos</span></span><br><span class="line">        w[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        w[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="comment"># 把这个预计算好的 w 塞进 nn.Embedding 里</span></span><br><span class="line">        self.emb = nn.Embedding(c_in, d_model)</span><br><span class="line">        self.emb.weight = nn.Parameter(w, requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.emb(x).detach() <span class="comment"># detach() 再次确保这些数值不会产生梯度影响</span></span><br></pre></td></tr></table></figure>
<p>这个和位置编码差不多，位置编码的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEmbedding, self).__init__()</span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model).<span class="built_in">float</span>()</span><br><span class="line">        pe.require_grad = <span class="literal">False</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = (torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model)).exp()</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 一个 PyTorch 小技巧。pe 矩阵不需要训练，但希望它随模型一起保存（.pth 文件）</span></span><br><span class="line">        <span class="comment"># 用 register_buffer 注册后，它就成了模型的一个固定配置，而不是需要优化的参数</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.pe[:, :x.size(<span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p>两者结合就能够让模型看到一个点，它就知道这是序列里的第 10 个点（来自 <code>PositionalEmbedding</code>）。24小时之后的同一位置也是一天里的上午 10 点（来自 <code>TemporalEmbedding</code>）。</p>
<h3 id="Encoder层"><a href="#Encoder层" class="headerlink" title="Encoder层"></a>Encoder层</h3><p>整体结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">self.encoder = Encoder(</span><br><span class="line">    [</span><br><span class="line">        EncoderLayer(</span><br><span class="line">            AttentionLayer(</span><br><span class="line">                FullAttention(<span class="literal">False</span>, configs.factor, attention_dropout=configs.dropout,</span><br><span class="line">                              output_attention=configs.output_attention),</span><br><span class="line">                configs.d_model, configs.n_heads),</span><br><span class="line">            configs.d_model,</span><br><span class="line">            configs.d_ff,</span><br><span class="line">            dropout=configs.dropout,</span><br><span class="line">            activation=configs.activation</span><br><span class="line">        ) <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(configs.e_layers)</span><br><span class="line">    ],</span><br><span class="line">    norm_layer=torch.nn.LayerNorm(configs.d_model)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>AttentionLayer</code>负责把 <code>FullAttention</code> 包装起来，管理多头（Multi-head）的投影矩阵 $W_q, W_k, W_v$。</p>
<p><code>EncoderLayer</code>能把注意力机制和前馈网络（FFN）组合在一起。</p>
<p><code>Encoder</code>是给所有的层加上了<code>LayerNorm</code></p>
<p>这两个和之前的源码差不多，里面的<code>FullAttention</code>不太一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FullAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, mask_flag=<span class="literal">True</span>, factor=<span class="number">5</span>, scale=<span class="literal">None</span>, attention_dropout=<span class="number">0.1</span>, output_attention=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FullAttention, self).__init__()</span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.mask_flag = mask_flag</span><br><span class="line">        self.output_attention = output_attention <span class="comment"># 意思是模型要不要把Softmax后的结果导出来</span></span><br><span class="line">        self.dropout = nn.Dropout(attention_dropout)</span><br><span class="line">	<span class="comment"># tau=None, delta=None这两个参数一个是用于调节 Softmax 的平滑度，一个是引入某种先验的相对位置偏置</span></span><br><span class="line">    <span class="comment"># 这个在Informer里面会用到，Transformer里面没有用到这个</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, attn_mask, tau=<span class="literal">None</span>, delta=<span class="literal">None</span></span>):</span><br><span class="line">        B, L, H, E = queries.shape</span><br><span class="line">        _, S, _, D = values.shape</span><br><span class="line">        scale = self.scale <span class="keyword">or</span> <span class="number">1.</span> / sqrt(E)</span><br><span class="line">		<span class="comment"># 在基础的 Transformer 实现中，我先用 transpose 把 Batch 和 Head 挪到前面，然后再做 matmul</span></span><br><span class="line">        <span class="comment"># 但是在工业级时序模型中往往用爱因斯坦求和约定（torch.einsum）</span></span><br><span class="line">        <span class="comment"># 逗号前后的字母：代表输入张量的维度。</span></span><br><span class="line">        <span class="comment"># 箭头后的字母：代表输出张量的维度。</span></span><br><span class="line">        <span class="comment"># 如果一个字母在逗号前出现了，但在箭头后消失了，说明这个维度被求和（相乘并相加）掉了</span></span><br><span class="line">        <span class="comment"># Query: (B, L, H, E) -&gt; 对应 blhe</span></span><br><span class="line">		<span class="comment"># Key: (B, S, H, E) -&gt; 对应 bshe</span></span><br><span class="line">		<span class="comment"># 输出 (B, H, L, S) -&gt; 对应 bhls</span></span><br><span class="line">        <span class="comment"># 字母 e 表示 Embedding 特征维度，在两个输入中都有，对于这种两者都有且对齐的，会进行了点积</span></span><br><span class="line">        <span class="comment"># 箭头后面消失了e，就说明它把 Query 的 e 维和 Key 的 e 维进行求和</span></span><br><span class="line">        <span class="comment"># 此外，他还带了一个重排功能，箭头后面的字母顺序，就是输出张量长成什么样，这个主要是为了求A方便，直接dim = -1即可</span></span><br><span class="line">        scores = torch.einsum(<span class="string">&quot;blhe,bshe-&gt;bhls&quot;</span>, queries, keys)</span><br><span class="line">		<span class="comment"># 传统方法也能实现，比较麻烦：</span></span><br><span class="line">        <span class="comment"># 原始维度: queries=[B, L, H, E], keys=[B, S, H, E]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一步：把 Head(H) 维度提到前面 </span></span><br><span class="line">        <span class="comment"># 因为 PyTorch 的矩阵乘法通常在最后两个维度进行，把 H 提出来可以实现“多头并行”</span></span><br><span class="line">        <span class="comment"># q = queries.permute(0, 2, 1, 3)  # [B, H, L, E]</span></span><br><span class="line">        <span class="comment"># k = keys.permute(0, 2, 1, 3)     # [B, H, S, E]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二步：对 Key 进行转置，准备做点积</span></span><br><span class="line">        <span class="comment"># 我们需要把 [B, H, S, E] 变成 [B, H, E, S]，这样才能和 [B, H, L, E] 乘起来</span></span><br><span class="line">        <span class="comment"># k_t = k.transpose(-1, -2)        # [B, H, E, S]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三步：批次矩阵乘法</span></span><br><span class="line">        <span class="comment"># 计算结果维度: [B, H, L, E] * [B, H, E, S] -&gt; [B, H, L, S]</span></span><br><span class="line">        <span class="comment"># scores = torch.matmul(q, k_t)    # 最终得到 [B, H, L, S]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 要不要做掩码</span></span><br><span class="line">        <span class="keyword">if</span> self.mask_flag:</span><br><span class="line">            <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                attn_mask = TriangularCausalMask(B, L, device=queries.device)</span><br><span class="line"></span><br><span class="line">            scores.masked_fill_(attn_mask.mask, -np.inf)</span><br><span class="line"></span><br><span class="line">        A = self.dropout(torch.softmax(scale * scores, dim=-<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 求V矩阵，还原维度，因为后面的残差链接等都希望是[B,L,H,D]这样的形状</span></span><br><span class="line">        V = torch.einsum(<span class="string">&quot;bhls,bshd-&gt;blhd&quot;</span>, A, values)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.output_attention:</span><br><span class="line">            <span class="keyword">return</span> (V.contiguous(), A)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (V.contiguous(), <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>这里用的是爱因斯坦求和约定（<code>torch.einsum</code>）方法，主要是他比较简单，有的时候用Transformer源码方法写复杂了很难处理，还有老方法中，张量在内存里是不连续的，经常要强制调用<code>.contiguous()</code>方法，容易报错。</p>
<h3 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h3><p>和<code>Encoder</code>很像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">self.decoder = Decoder(</span><br><span class="line">    [</span><br><span class="line">        DecoderLayer(</span><br><span class="line">            AttentionLayer(</span><br><span class="line">                FullAttention(<span class="literal">True</span>, configs.factor, attention_dropout=configs.dropout,</span><br><span class="line">                              output_attention=<span class="literal">False</span>),</span><br><span class="line">                configs.d_model, configs.n_heads),</span><br><span class="line">            AttentionLayer(</span><br><span class="line">                FullAttention(<span class="literal">False</span>, configs.factor, attention_dropout=configs.dropout,</span><br><span class="line">                              output_attention=<span class="literal">False</span>),</span><br><span class="line">                configs.d_model, configs.n_heads),</span><br><span class="line">            configs.d_model,</span><br><span class="line">            configs.d_ff,</span><br><span class="line">            dropout=configs.dropout,</span><br><span class="line">            activation=configs.activation,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(configs.d_layers)</span><br><span class="line">    ],</span><br><span class="line">    norm_layer=torch.nn.LayerNorm(configs.d_model),</span><br><span class="line">    projection=nn.Linear(configs.d_model, configs.c_out, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在 <code>DecoderLayer</code> 内部，定义了两个 <code>AttentionLayer</code>。第一个是Self-Attention，第二个是Cross-Attention它不需要 Mask。用<code>DecoderLayer</code>打包，然后定义了<code>norm_layer</code>层归一化，最后线性投影输出，用projection把<code>d_model</code> 压回<code>c_out</code> 维度。</p>
<h3 id="forecast"><a href="#forecast" class="headerlink" title="forecast"></a>forecast</h3><p>整合起来，用<code>forecast</code>进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forecast</span>(<span class="params">self, x_enc, x_mark_enc, x_dec, x_mark_dec</span>):</span><br><span class="line">    enc_out = self.enc_embedding(x_enc, x_mark_enc)</span><br><span class="line">    enc_out, attns = self.encoder(enc_out, attn_mask=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    dec_out = self.dec_embedding(x_dec, x_mark_dec)</span><br><span class="line">    dec_out = self.decoder(dec_out, enc_out, x_mask=<span class="literal">None</span>, cross_mask=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">return</span> dec_out</span><br></pre></td></tr></table></figure>
<p>以单变量预测为例，告诉96小时的历史数据，Teacher Forcing 为48小时，预测72小时数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量名</th>
<th>物理含义</th>
<th>形状 (Batch, Time, Dim)</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>x_enc</code></td>
<td>历史观测值</td>
<td><code>[B, 96, c_in]</code></td>
<td>模型预测的依据（比如过去 96 小时的热负荷数值）。</td>
</tr>
<tr>
<td><code>x_mark_enc</code></td>
<td>历史时间戳</td>
<td><code>[B, 96, 4]</code></td>
<td>告诉模型过去 96 小时分别对应周几、几点。</td>
</tr>
<tr>
<td><code>x_dec</code></td>
<td>引导输入</td>
<td><code>[B, 48+72, c_in]</code></td>
<td>包含两部分：一部分是已知的历史（超参数中的<code>Label_len</code>），一部分是待填充的未来（通常填 0）。</td>
</tr>
<tr>
<td><code>x_mark_dec</code></td>
<td>未来时间戳</td>
<td><code>[B, 48+72, 4]</code></td>
<td>这也要提前告知的。因为即使不知道未来的数值，但都是知道未来是周几、几点。</td>
</tr>
</tbody>
</table>
</div>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">z01prime</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="http://z01prime.github.io/2026/01/24/注意力机制/">http://z01prime.github.io/2026/01/24/注意力机制/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">z01prime</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2026/01/16/%E4%B8%89%E8%93%9D%E4%B8%80%E6%A3%95%EF%BC%9A%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" title="三蓝一棕：傅里叶变换"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">三蓝一棕：傅里叶变换</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">z01prime</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/z01prime"><i class="fab fa-github"></i><span>我的github主页</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/z01prime" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:z7296091@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">加载慢可以挂个梯子试试捏</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">重要的概念</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">2.</span> <span class="toc-text">注意力机制的直观理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Token"><span class="toc-number">2.1.</span> <span class="toc-text">Token</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86Token%E5%85%B3%E8%81%94%E5%88%B0%E5%B5%8C%E5%85%A5%E5%90%91%E9%87%8F%E7%9A%84%E9%AB%98%E7%BB%B4%E5%90%91%E9%87%8F%E3%80%82"><span class="toc-number">2.2.</span> <span class="toc-text">将Token关联到嵌入向量的高维向量。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W-Q-%E7%9F%A9%E9%98%B5"><span class="toc-number">2.3.</span> <span class="toc-text">$W_Q$矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W-K-%E7%9F%A9%E9%98%B5"><span class="toc-number">2.4.</span> <span class="toc-text">$W_K$矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A9%E7%A0%81"><span class="toc-number">2.5.</span> <span class="toc-text">掩码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W-V-%E7%9F%A9%E9%98%B5"><span class="toc-number">2.6.</span> <span class="toc-text">$W_V$矩阵</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">多层感知机&#x2F;前馈神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GPT3%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="toc-number">4.</span> <span class="toc-text">GPT3的参数量计算</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB"><span class="toc-number">5.</span> <span class="toc-text">Transformer源码解读</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-number">5.1.</span> <span class="toc-text">环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E4%B8%80%E4%BA%9B%E5%8C%85"><span class="toc-number">5.2.</span> <span class="toc-text">导入一些包</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">5.3.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8%E4%B8%8ETarget-Source%E9%95%BF%E5%BA%A6"><span class="toc-number">5.4.</span> <span class="toc-text">词表与Target&#x2F;Source长度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8"><span class="toc-number">5.4.1.</span> <span class="toc-text">词表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Target-Source%E9%95%BF%E5%BA%A6"><span class="toc-number">5.4.2.</span> <span class="toc-text">Target&#x2F;Source长度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%EF%BC%9A"><span class="toc-number">5.5.</span> <span class="toc-text">超参数设置：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">5.6.</span> <span class="toc-text">模型代码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">5.6.1.</span> <span class="toc-text">多头注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.6.2.</span> <span class="toc-text">前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder%E7%9B%B8%E5%85%B3"><span class="toc-number">5.6.3.</span> <span class="toc-text">Encoder相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dncoder%E7%9B%B8%E5%85%B3"><span class="toc-number">5.6.4.</span> <span class="toc-text">Dncoder相关</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E4%B8%BB%E7%B1%BB"><span class="toc-number">5.6.5.</span> <span class="toc-text">Transformer主类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%BB%E5%9B%BE%E5%87%BD%E6%95%B0"><span class="toc-number">5.7.</span> <span class="toc-text">画图函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">5.8.</span> <span class="toc-text">模型训练与结果可视化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer4TS"><span class="toc-number">6.</span> <span class="toc-text">Transformer4TS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%E7%9A%84%E8%B6%85%E5%8F%82%E6%95%B0%E8%A7%A3%E8%AF%BB"><span class="toc-number">6.1.</span> <span class="toc-text">启动脚本的超参数解读</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB"><span class="toc-number">6.2.</span> <span class="toc-text">Transformer模型代码解读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedding%E5%B1%82"><span class="toc-number">6.2.1.</span> <span class="toc-text">Embedding层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder%E5%B1%82"><span class="toc-number">6.2.2.</span> <span class="toc-text">Encoder层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder%E5%B1%82"><span class="toc-number">6.2.3.</span> <span class="toc-text">Decoder层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#forecast"><span class="toc-number">6.2.4.</span> <span class="toc-text">forecast</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="注意力机制">注意力机制</a><time datetime="2026-01-24T02:30:50.000Z" title="发表于 2026-01-24 10:30:50">2026-01-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/16/%E4%B8%89%E8%93%9D%E4%B8%80%E6%A3%95%EF%BC%9A%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" title="三蓝一棕：傅里叶变换">三蓝一棕：傅里叶变换</a><time datetime="2026-01-16T02:16:47.000Z" title="发表于 2026-01-16 10:16:47">2026-01-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/22/%E5%85%B3%E4%BA%8E%E9%9B%85%E6%80%9D/" title="关于雅思">关于雅思</a><time datetime="2025-12-22T02:29:43.000Z" title="发表于 2025-12-22 10:29:43">2025-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/21/2021SCCPC/" title="2021SCCPC">2021SCCPC</a><time datetime="2025-04-21T06:40:56.000Z" title="发表于 2025-04-21 14:40:56">2025-04-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/31/2024CCPC%E5%B1%B1%E4%B8%9C%E9%82%80%E8%AF%B7%E8%B5%9B/" title="2024CCPC山东邀请赛">2024CCPC山东邀请赛</a><time datetime="2024-12-31T07:43:10.000Z" title="发表于 2024-12-31 15:43:10">2024-12-31</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://source.fomal.cc/img/dm8.webp')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By z01prime</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'e8f61b4db601b696558e',
      clientSecret: '2f5cacaf3b111d5db02b1b40be93f02553bf9aa4',
      repo: 'commetForGitalk',
      owner: 'z01prime',
      admin: ['z01prime'],
      id: '654fa1289a125100ee5c061238a41edb',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.textContent= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="25" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>