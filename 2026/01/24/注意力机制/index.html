<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>注意力机制 | z01prime</title><meta name="author" content="z01prime"><meta name="copyright" content="z01prime"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="动图轻松理解Self-Attention(自注意力机制) - 知乎 Attention in transformers, step-by-step | Deep Learning Chapter 6 How might LLMs store facts | Deep Learning Chapter 7 重要的概念query、key和value。这三个词翻译成中文就是查询、键、值。 可以理解为一种">
<meta property="og:type" content="article">
<meta property="og:title" content="注意力机制">
<meta property="og:url" content="http://example.com/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html">
<meta property="og:site_name" content="z01prime">
<meta property="og:description" content="动图轻松理解Self-Attention(自注意力机制) - 知乎 Attention in transformers, step-by-step | Deep Learning Chapter 6 How might LLMs store facts | Deep Learning Chapter 7 重要的概念query、key和value。这三个词翻译成中文就是查询、键、值。 可以理解为一种">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp">
<meta property="article:published_time" content="2026-01-24T02:30:50.000Z">
<meta property="article:modified_time" content="2026-01-25T04:09:28.687Z">
<meta property="article:author" content="z01prime">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":700},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '注意力机制',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2026-01-25 12:09:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 传送门</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://source.fomal.cc/img/dm8.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="z01prime"><span class="site-name">z01prime</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 传送门</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">注意力机制</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-24T02:30:50.000Z" title="发表于 2026-01-24 10:30:50">2026-01-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-25T04:09:28.687Z" title="更新于 2026-01-25 12:09:28">2026-01-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/">时间序列预测</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="注意力机制"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/619154409">动图轻松理解Self-Attention(自注意力机制) - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=eMlx5fFNoYc&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=7">Attention in transformers, step-by-step | Deep Learning Chapter 6</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9-Jl0dxWQs8&amp;t=1190s">How might LLMs store facts | Deep Learning Chapter 7</a></p>
<h1 id="重要的概念"><a href="#重要的概念" class="headerlink" title="重要的概念"></a>重要的概念</h1><p><strong>query、key和value</strong>。这三个词翻译成中文就是<strong>查询、键、值</strong>。</p>
<p>可以理解为一种行为：当厨师在锅里准备放下一个调料时，他会看一眼菜谱。书会将他的查询<strong>query</strong>映射到书中相关的标签<strong>key</strong>，如辣椒，盐等等，然后书中会展示最匹配的调料<strong>value</strong>。</p>
<p><strong>Source</strong> 和 <strong>Target</strong>。</p>
<p><strong>Source（源）</strong>：指的是输入。这里是菜谱。</p>
<p><strong>Target（目标）</strong>：指的是输出。这里是锅里正在炒的菜（以及厨师脑子里已经生成的做菜步骤）</p>
<p><strong>Self-Attention</strong>与<strong>Attention</strong>。</p>
<p>上述的行为其实是属于 <strong>Attention（或者叫 Cross-Attention）</strong>，而不是 Self-Attention。Attention 是“拿着钥匙（Query）去开别人家的锁（Key）”。</p>
<p>Self-Attention 是“把家里所有的零件（Q, K, V）摆在地上，看看谁和谁能拼在一起”。</p>
<ol>
<li>类比：厨师做菜（Cross-Attention）</li>
</ol>
<p>在这个场景中：</p>
<ul>
<li>Source是<strong>菜单/菜谱</strong>（上面写着：宫保鸡丁 -&gt; 鸡肉、花生、辣椒）。</li>
<li>Target是<strong>厨师锅里的菜</strong>。</li>
</ul>
<p><strong>Cross-Attention 发生的过程：</strong> 当厨师（模型）在锅里准备放下一个调料（生成下一个词）时，他会抬头看一眼菜单（Source）。</p>
<ul>
<li><strong>Query</strong>：厨师：“我现在锅里有鸡肉了，下一步该放什么？”</li>
<li><strong>Key</strong>：菜单上的每一行标题（如“主料“，调味”）。</li>
<li><strong>Value</strong>：菜单上对应的具体内容（如“葱段”、“盐”）。</li>
</ul>
<p><strong>结论</strong>：厨师通过 Query 去匹配菜单的 Key，最后拿到了 Value 放入锅中。这就是Target（锅里的进展）对 Source（静态菜单）的注意力。</p>
<ol>
<li>类比：理解食材（Self-Attention）</li>
</ol>
<p>在厨师还没开始炒菜，只是阅读菜单的时候，发生的是Self-Attention。</p>
<ul>
<li><p>菜单上写着“将洗净的<strong>辣椒</strong>切段，因为它很<strong>辣</strong>”。</p>
</li>
<li><p>为了理解这个“它”是指辣椒还是指手，厨师的脑子会自动把“它”和“辣椒”关联起来。</p>
</li>
<li><p>这是Source 内部元素之间的互看。“它”产生的 Query 向量，在空间中的方向会指向“寻找那些具有名词属性且容易辣的东西”。</p>
<p>而“辣椒”产生的 Key 向量，正好在空间中的那个位置。</p>
</li>
</ul>
<h1 id="注意力机制的直观理解"><a href="#注意力机制的直观理解" class="headerlink" title="注意力机制的直观理解"></a>注意力机制的直观理解</h1><p>接下来的学习会以文本生成为例。</p>
<h2 id="Token"><a href="#Token" class="headerlink" title="Token"></a><strong>Token</strong></h2><p>指的是文本被切割成的小块，一般是单词或者是单词片段。</p>
<p>注意，Token 不是由Transformer模型本身分割的，而是由一个专门的预处理工具——分词器（Tokenizer）完成的。</p>
<p>分词器是在数据进入模型之前独立工作的。先把长文本切成标准规格的小块，再送进Transformer。</p>
<p>现在的主流模型（如 GPT-4, Llama）既不按字符切，也不完全按单词切，而是采用一种折中的方案。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>例子：unhappiness</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Word-level (按词)</strong></td>
<td><code>unhappiness</code></td>
<td>含义明确</td>
<td>词表太大（几百万个词），遇到新词（如网络流行语）就傻眼。</td>
</tr>
<tr>
<td><strong>Character-level (按字符)</strong></td>
<td><code>u, n, h, a, p...</code></td>
<td>词表极小（几十个字母）</td>
<td>字符本身没意义，序列太长，增加模型计算负担。</td>
</tr>
<tr>
<td><strong>Subword-level (子词 - 主流)</strong></td>
<td><code>un, happi, ness</code></td>
<td><strong>兼顾两者</strong></td>
<td>将常见词保留完整，生僻词拆成碎块。</td>
</tr>
</tbody>
</table>
</div>
<p>这个主流分法有点像词根词缀的分法，但是这其实是一种统计意义上的分法，在结果上确实表现得非常像“词根词缀”拆解法。人类看到 unhappiness，我们知道 un- 是否定前缀，happy 是词根，-ness 是名词后缀。而分词器是扫描了数以亿计的文本，发现 un 这个组合出现的频率高得离谱，ness 也是。于是它会认为两个碎块很有用，要把它们存进词表里。</p>
<p>所以这种分法往往与语言学结构高度吻合。但是事实上，如果the经常出现在there里，而re也是高频碎块，它可能会把there拆成the和 re。这在语言学上是错的，但对模型来说，只要能降低词表大小且覆盖所有字符就行。</p>
<p>用到了分词主要是为了建立数学联系：当两个向量在空间里靠得近，它们就产生了注意力。</p>
<h2 id="将Token关联到嵌入向量的高维向量。"><a href="#将Token关联到嵌入向量的高维向量。" class="headerlink" title="将Token关联到嵌入向量的高维向量。"></a>将Token关联到嵌入向量的高维向量。</h2><p>简单来说，嵌入向量（Embedding Vector）就是单词在模型中的表示，模型会把所有的单词转换为坐标。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123192231878.png" alt="image-20260123192231878" style="zoom:50%;"></p>
<p>在这一张图中，每一个单词都被转换成了上千个数字，每一个数字都可以被理解为一个属性维度，而大小则可以被理解为它们代表了在某个抽象维度上的偏移。虽然模型内部的维度很难用人类语言精准描述，但我们可以为了直观理解进行类比。假设一个只有 3 维的嵌入向量：一维表示的是是否是生物（-1到1），二维是体积大小，三维是是否昂贵。那么有：</p>
<ul>
<li>猫”可能是[0.9, -0.5, -0.2]</li>
<li>“狗” 可能是[0.9, -0.1, -0.3]（因为都是生物，第一项很接近）</li>
<li>“手机”可能是[-0.8, -0.6, 0.7]（非生物，且比较贵）</li>
</ul>
<p>而Transformer会计算这些向量之间的角度。如果两个向量指向的方向差不多，模型就认为它们之间需要产生强烈的注意力。</p>
<p>这个向量是怎么来的？它是练出来的。模型刚开始完全是乱猜的，给每个词分配一串随机数字。模型在读了海量文本后发现，猫和狗经常出现在宠物、吃、可爱这种词周围。为了让预测更准，模型会自动调整数字，让猫和狗的向量在坐标系里靠得非常近。</p>
<p>注意力模块不仅细化了一个词的含义，还允许模型互相传递这些嵌入向量所蕴含的信息。</p>
<p>在没有注意力模块之前，每个词的向量是孤立的。比如单词 Bank。训练好的向量包含了一点 银行 的意思，也包含了一点 河岸 的意思，是个模糊的混合体。注意力模块的作用是，如果周围的词有 Money 、 Interest ，注意力模块会把这些词向量中的金融特征提取出来，将这些信息组合为向量，加到 Bank 的向量中。此时 Bank 的向量被在空间中的位置发生了偏移，变成了一个纯粹代表 银行 的向量。这个例子中的K Q V：</p>
<p>Query (Q) : Bank发出了信号，需要知道Bank在这个上下文中的具体意义，在数学意义上，Q是针对当前词想要关注的方向。</p>
<p>Key (K)：是Money/Interest的标签，是由由周围的词（如 Money）产生。数学上K用来和Q做点积。如果Q（想要钱的信息）和K（我是钱）匹配度高，计算出来的注意力权重（Score）就会非常大。</p>
<p>Value (V)：Money/Interest 传递的实质信息。既然匹配上了，Money 就把它的“金融属性”打包成一个向量发过去。这是最终被融合到 Bank 向量上的那部分实质内容。</p>
<p>注意，<strong>单纯的词嵌入向量是不含位置信息的</strong>。而在实际的预测过程中，模型还会加上位置向量，代表了这个Token在一个句子中的位置，因为如果只给模型一堆词向量，模型分不清“狗咬人”和“人咬狗”，因为对模型来说这只是两个完全一样的向量集合。所以在这些词向量进入注意力模块之前，模型会给每个向量强行加上一个微小的、特制的位置向量（Positional Encoding）。比如I saw a saw.第一个saw是动词，而第二个就是名词，会有区别，他们的数值就会产生细微差别。这样模型就能通过数学计算知道这个词应该出现在哪一个位置。</p>
<p>有了这些带有位置和基础语义的向量，就可以通过<strong>$W_Q, W_K, W_V$</strong>矩阵进行线性变换。</p>
<h2 id="W-Q-矩阵"><a href="#W-Q-矩阵" class="headerlink" title="$W_Q$矩阵"></a>$W_Q$矩阵</h2><p>$W_Q$ 是一个具体任务。Transformer 并不是只有一个 $W_Q$，它拥有成百上千个并行的微型任务。因为Transformer是多头注意力机制，所以第一个头可能在问这个词的前面形容词的个数，另一个可能是在问前面名词个数，这里说是任务，实际上在数学中，他更像是一个视角，他放大了这个词的一些属性。在训练时，它是动态的。 模型通过不断看大量的句子，利用梯度下降算法去反复调整 $W_Q$ 矩阵里的每一个数字。在推理/使用时，它是“静态”的。一旦模型练好了，这个 $W_Q$ 矩阵里的数值就固定死了。</p>
<h2 id="W-K-矩阵"><a href="#W-K-矩阵" class="headerlink" title="$W_K$矩阵"></a>$W_K$矩阵</h2><p>$W_K$ 的作用就是将句子中每一个词的原始向量，也转换到一个同样的抽象任务空间里，把它变成一张方便被索引的名片（Key）。$W_Q$（Query）给出了一个具体任务，那么$W_K$（Key）就是每一个Token这个任务的具体的权重。具体来说，以“狗”这个词为例：</p>
<p>Embedding：它是“狗”这个词的各种属性分解，可能包含“动物”、“尺寸”、“寿命”、“忠诚度”等属性，将他压缩成了一串数字，称为向量。</p>
<p><strong>$W_Q$</strong>：它不是在制造属性，而是在筛选属性。它定义了在当前的注意力头里，我们只关心动物性和位置。它把这两者的权重调高，让 $Q$ 变成了一个带有强烈搜索指向的向量。</p>
<p><strong>$W_K$</strong>：它让所有的词（不仅仅是狗）都根据 $W_Q$ 的偏好来重新表达。它把所有词的 Embedding 转化为不同的 $K$，这个 $K$ 重点突出了“我是动物”这个标签，以便能被 $Q$ 发现。</p>
<p><strong>$Q \cdot K$</strong>：这步运算就是属性吻合度检测，当需求（Q）和拥有（K）在某些维度上同时呈现出高分，点积就会变得巨大。这个分值就是“注意力权重”。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123213100771.png" alt="image-20260123213100771" style="zoom:50%;"></p>
<p>这张图是假设$W_Q$为：有没有哪一个形容词在我前面。就会产生这样的结果，相乘结果越大说明得分越高，也就是越相关。</p>
<p>为什么要搞得这么复杂？既然 Embedding 里已经有“动物”属性了，为什么不能直接拿来比？答案是为了上下文灵活性。在“狗咬人”里，模型需要注意狗的“攻击性”。在“狗很可爱”里，模型需要注意狗的“外观属性”。同一个 Embedding，通过不同的 $W_Q$ 和 $W_K$（不同的头），可以在第一种情况下提取“攻击性”进行匹配，在第二种情况下提取“可爱度”进行匹配。</p>
<p>这样模型就能够根据语境理解单词。</p>
<p><strong>$W_Q$</strong> 定义了“任务”：在这个视角下，哪些属性是加分项。</p>
<p><strong>$W_K$</strong> 提取了“名片”：把词的原始含义转化成该视角下的属性值。</p>
<p><strong>$Q \cdot K$</strong> 实现了“匹配”：属性越吻合，分值越高，产生的权重就越大。</p>
<p>回到那张图，然后将这些经过SoftMax后得到：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123213845905.png" alt="image-20260123213845905" style="zoom:67%;"></p>
<p>这个叫做注意力模式(Attention Pattern)，在论文中被表示为一个公式：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123214116705.png" alt="image-20260123214116705" style="zoom:67%;"></p>
<p>这里面有一个V，先不考虑，$\sqrt{d_k}$被称为缩放因子（Scaling Factor），它的出现是为了解决一个数学问题：防止梯度消失。</p>
<p>$d_k$ 代表Key向量的维度。简单来说，就是向量里有多少个数字。</p>
<p>要除以它的平方根是因为Softmax对巨大的数值非常不敏感，点积会随着维度变大而爆炸，假设向量里的每个数字平均大小是 1。如果向量只有 2 维，点积结果大概是 2；但如果向量有 1000 维，点积结果可能就会冲到 1000 左右。Softmax 函数在输入数值非常大（比如 100 或 1000）时，其梯度（导数）会变得极其微小，几乎接近于 0。如果分数太高，Softmax 算出来的概率会变成极其极端的 $[0.9999…, 0.0000…]$。一旦进入这个状态，模型在训练时就无法通过梯度来更新权重了。也叫梯度消失。通过除以 $\sqrt{d_k}$，模型强行把那些可能“爆炸”的点积得分拉回到一个适中的范围。</p>
<h2 id="掩码"><a href="#掩码" class="headerlink" title="掩码"></a>掩码</h2><p>此外还有一个细节，在训练的过程中，对于给定的示例文本训练时，模型会预测出下一个词的概率高低，然后最小化损失函数来调整权重，但是更效率的做法是同时预测每一个初始Token子序列的下一个Token，比如可以同时预测第一个‘the’的后一个Token，也可以预测‘creature’的下一个Token，这样每一个训练样本可以提供多次训练机会，这也是Transformer训练比老一代模型（如 RNN）快得多的原因：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124104808697.png" alt="image-20260124104808697" style="zoom:67%;"></p>
<p>但是存在一个问题，我们需要让模型看不到后面的词，不然后面的词会影响前面的词，存在数据泄漏。所以在softmax之前，往往会将后面的值初始化为负无穷：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124105219170.png" alt="image-20260124105219170" style="zoom:50%;"></p>
<p>这在训练阶段是必须防止的。但其实在测试/生成阶段（比如和已经训练好的模型聊天时），其实不需要专门设置负无穷，因为那时候未来的词根本还没生出来，模型想抄也抄不到。GPT 系列 (ChatGPT, Llama, Claude)这类模型会在两个阶段都设置掩码，但是像是BERT这类模型在任何阶段都不使用这种防偷看的掩码。</p>
<p>这个是在自注意力机制下的，如果是交叉注意力，那么其实根本就不用管掩码的问题。</p>
<h2 id="W-V-矩阵"><a href="#W-V-矩阵" class="headerlink" title="$W_V$矩阵"></a>$W_V$矩阵</h2><p>然后$V$（Value）其实就是负责怎么把实质内容搬运过去的指令集。</p>
<p>为什么不直接加原向量，而要用 $V$ 转换一下？</p>
<p>$V$ 的作用是告诉模型应该搬运什么信息。它能够过滤杂质，因为原始的 Embedding $\vec{E}$ 包含了太多的信息（比如这个词的词性、位置、首字母是什么等）。但在当前的注意力任务中（比如“寻找前面的形容词”），<code>creature</code> 只需要 <code>blue</code> 这个词中关于“蓝色”的具体语义。$W_V$ 矩阵也是一个信息提取器。它把 <code>blue</code> 的原始向量中那些对当前任务有意义的实质性内容提取出来，变成了 $\vec{V}$。</p>
<p> $V$ 与 $K$ 的区别，其实是标签与内容的区别，具体来说$K$ (Key)：是给别人看的，为了让 $Q$ 找到自己，它决定了权重是多少。$V$ (Value)：是给别人用的（为了让 $Q$ 吸收自己），它决定了搬运的内容是什么。比如去买书，书的封面和标题是 $K$，它是吸引买家买下它的理由（产生权重）；书里的文字内容是 $V$，他是最后带回家读进脑子里的东西。数学上是加权求和实现的，$Q$ 和 $K$ 算出了权重（比如 $0.58$）。通过$W_V$ 把该词变成 $\vec{V}$。然后将 $\vec{V}$ 乘以 $0.58$。最后把这个缩放后的向量加到产生 $Q$ 的那个词（<code>creature</code>）的 $\vec{E}_4$ 向量上。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260123224816601.png" alt="image-20260123224816601" style="zoom: 50%;"></p>
<p>在自注意力机制中，每一个词都在同时扮演“提问者”和“被选者”的角色。在这一层处理结束时，不仅仅是 <code>creature</code> 变聪明了，句子里的每一个词都根据上下文完成了更新：</p>
<ul>
<li><strong>对于 <code>creature</code></strong>：它吸收了 <code>fluffy</code> 和 <code>blue</code> 的信息，变成了“蓬松的蓝色的生物”。</li>
<li><strong>对于 <code>forest</code></strong>：它可能通过自己的 $Q$ 找到了前面的 <code>verdant</code>（翠绿的），从而吸收了“绿色”的特征，变成了“翠绿的森林”。</li>
<li><strong>对于 <code>roamed</code></strong>：它可能关注到了 <code>creature</code>，从而在向量里补充了“是谁在漫游”的信息。</li>
</ul>
<p>这种全员同步更新的过程，就是图中下方那一排 $\Delta \vec{E}_1, \Delta \vec{E}_2 \dots$ 的含义。每一个词都算出了一个所有词的权重，然后加回到自己原来的向量上，生成了全新的 $\vec{E}’$。</p>
<p>注意这个是单头注意力的情况，它只代表了模型一种观察句子的方式，但是如果只用这一个矩阵，模型可能就没精力去管“主谓关系”或“代词指代”了。这就是为什么真正的 Transformer 是多头的：它们会并行运行好几个这样的内容，每个头都有自己的 $W_Q, W_K, W_V$。比如头 1让名词找形容词；头 2 让动词找主语；头 3找逻辑转折词。</p>
<p>右侧公式$\vec{E}_n + \Delta \vec{E}_n = \vec{E}’_n$中，</p>
<ol>
<li>$\vec{E}_n$ (原始值)：保持了词语原本的含义（比如“我是个生物”）。</li>
<li>$\Delta \vec{E}_n$ (变化量)：这是通过刚才那一堆 $Q, K, V$ 计算出来的上下文中需要注意的内容。</li>
<li>$\vec{E}’_n$ (最终结果)：这是带有上下文理解的词向量。</li>
</ol>
<p>这种“加法”操作在深度学习里叫残差连接（Residual Connection）。它的好处在于即便注意力机制算错了，原始的词意也不会丢失，模型只会把注意力看作一种“补充修正”。这个概念最早来自 2015 年的 ResNet 论文。在数学上，如果你想让一个函数 $f(x)$ 学习某种变换，直接学 $f(x)$ 可能很难。但如果我们将输出定义为 $H(x) = x + f(x)$，那么 $f(x)$ 只需要学习输出与输入之间的差值（即 $H(x) - x$）。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095005646.png" alt="image-20260125095005646" style="zoom:50%;"></p>
<p>以GPT-3为例，每个模块内使用96个注意力头，即有96个不同的键和查询矩阵，会产生96种注意力模式，每个注意力头都会有独特的值矩阵，用于产生96个值向量序列，这张图中的中间部分展示了每一个token，每个头都会给出要加入到这个位置的嵌入中的变化量，所以对于原始的嵌入，要加上所有注意力头给出的变化量。</p>
<p>在具体实现中，还有一个细节，之前提到为了增加运行效率，可以对这个大矩阵进行低秩分解：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124114735640.png" alt="image-20260124114735640" style="zoom:50%;"></p>
<p>简单来说，模型并没有直接使用一个包含 $1.5$ 亿参数（$12,288^2$）的大矩阵，而是把它拆成了两个“瘦长”的矩阵相乘：</p>
<ul>
<li><strong>矩阵 A ($Value_\downarrow$)</strong>：大小为 $128 \times 12,288$（将高维压缩到低维）。</li>
<li><strong>矩阵 B ($Value_\uparrow$)</strong>：大小为 $12,288 \times 128$（将低维重新映射回高维）。</li>
</ul>
<p>$W_V$ 的效果 $\approx$ 矩阵 B $\times$ 矩阵 A，这种低秩分解（Low-rank Decomposition）和 SVD（奇异值分解）有点像，不同的是SVD 分解：通常是指已经有一个现成的大矩阵，为了分析它或者压缩它，用数学手段把它拆成三个矩阵的乘积 ($A = U\Sigma V^T$)。这会保留前 $k$ 个最大的奇异值，从而得到一个低秩的近似矩阵。而Transformer 中的低秩设计是，模型在设计之初，就根本没打算创建一个完整的大矩阵。工程师直接让模型去学习两个小的瘦长矩阵（矩阵 A 和 矩阵 B）。因此对每一个注意力头的$W_V$都做了一个分割：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095036229.png" alt="image-20260125095036229" style="zoom:50%;"></p>
<p>但是实际实现中$Value\uparrow$和$Value\downarrow$的说法会有所变化，所有的$Value\uparrow$会合在一起说成是输出矩阵，与整个注意力模块相关联，而每一个注意力头的$W_V$其实是$Value\downarrow$矩阵，即那些将嵌入向量投影到低纬度空间的矩阵。</p>
<h1 id="多层感知机-前馈神经网络"><a href="#多层感知机-前馈神经网络" class="headerlink" title="多层感知机/前馈神经网络"></a>多层感知机/前馈神经网络</h1><p>这部分占了大部分的模型参数。其实和传统的模型差不多，大致意思就是把信息打散了存在某一块里面。</p>
<p>有理论认为，这一块提供了额外的容量来存储事实。</p>
<p>以“迈克尔乔丹打篮球”这一事实为例，假设这个高维空间里面，有一个向量能够表示名字是迈克尔，另外有一个向量表示姓氏是乔丹，并且这两个向量几乎是垂直的。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095423417.png" alt="image-20260125095423417" style="zoom:50%;"></p>
<p>视频里面说垂直的原因在于，在高维空间中，无数个互不干扰的方向，非常空旷，而随机抽取两个向量，它们之间夹角接近 $90^\circ$ 的概率极高。显然，对于一个一个N维空间，最多只有N个两两垂直的向量，但是放宽一点限制，比如89°、91°也算正交，那么这些向量的数量是随维度的增加指数级增长的。这被称为高维空间的近似正交性，Johnson-Lindenstrauss (JL) 引理：在 $N$ 维空间中，你可以找到多达 $e^{N \cdot \epsilon^2}$ 个向量，它们两两之间的夹角都在 $90^\circ \pm \epsilon$ 之间。所以当从高维空间随机采样向量时，这些向量的分量往往表现出正态分布的特性，而这个特性会导致近似正交，证明：</p>
<p>因为在 $N$ 维空间有两个随机单位向量 $\vec{A}$ 和 $\vec{B}$。它们的内积（点积）决定了它们是否交：</p>
<script type="math/tex; mode=display">
\vec{A} \cdot \vec{B} = \sum_{i=1}^{N} a_i b_i</script><p>由于每个分量 $a_i$ 和 $b_i$ 都近似服从均值为 0 的正态分布：</p>
<ol>
<li>每一项 $a_i b_i$ 也是一个随机变量，其均值依然是 0。</li>
<li>当把这 $N$ 个项加起来时，根据大数定律，正值和负值会极大概率相互抵消。</li>
<li>随着维度 $N$ 越来越大，这个总和（内积）会极其趋近于 0。</li>
</ol>
<p>而内积趋近于 0，意味着夹角趋近于 $90^\circ$。</p>
<p>其实在机器学习中，通常都是希望模型学到的特征是独立且不相关的。这个可以说是模型训练后的理想结果；此外，模型在梯度下降时，如果发现两个概念混淆导致了误差，它也会这两个向量在空间中尽可能的正交，直到它们互不干扰（即趋向垂直）。这也是为什么再后面降到MLP的具体设计中，要把词向量进行升维。</p>
<p>如果有一个向量，表示迈克尔乔丹这个人，那么他与姓名这两个向量的点积一定是1（这里是假设1表示完美的信号匹配，在真实的 Transformer 模型中，点积经常大于 1，如果点积大于 1，通常意味着该特征被强化了，这也是为什么注意力机制公式里会看到 $\frac{QK^T}{\sqrt{d_k}}$。那个分母 $\sqrt{d_k}$ 就是为了把可能由于高维产生的巨大点积数值“压”回来，防止它太大导致 Softmax 饱和）。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095438986.png" alt="image-20260125095438986" style="zoom:50%;"></p>
<p>此外还有一个假设：由于“迈克尔·乔丹”这个文本跨越了两个Token（一个是左边的迈克尔，另一个是右边的乔丹），他要假设前面的注意力模块，成功将信息传递给这两个向量中的第二个（乔丹模块）确保能编码全名。</p>
<p>这个假设实际上是在解释模型如何处理分布式特征。在文本中，“Michael” 和 “Jordan” 是两个独立的 Token。在第一层，Michael 对应的向量只知道自己是“迈克尔”，Jordan 对应的向量只知道自己是“乔丹”。而MLP 层是逐位置计算的，它不会看旁边的词。如果信息不传递，Jordan 位置的 MLP 永远不知道前面站着一个 Michael，也就无法触发“篮球飞人”这个特定事实的提取。因此，必须有一个前置步骤，把分散的信息“汇聚”到一个地方。</p>
<p>在MLP里面，每一个向量都会通过一些操作，最终会得到另一个维数相同的向量，将得到的向量与原本的向量增加，而增加的结果就是输出值。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095451202.png" alt="image-20260125095451202" style="zoom:50%;"></p>
<p>注意这个时候，向量之间是不会交流的。</p>
<p>在刚才的例子中，注意力模块发现“乔丹”和“迈克尔”关系紧密，于是把“迈克尔”这个向量的信息拷贝并叠加到了“乔丹”的向量上。得到了一个编码了“名字迈克尔”和“姓氏乔丹”的向量，这个向量流入MLP，经过这一系列的运算，能够输出包含“篮球”方向的向量，再将其与原向量相加，就会得到输出向量。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095500370.png" alt="image-20260125095500370" style="zoom:50%;"></p>
<p>MLP 的第一层矩阵是线性变换 (Linear)。其中某一个神经元（一组权重）专门在等一个特定的信号：即“名=迈克尔”且“姓=乔丹”。如果这个混合向量里的两个信号同时足够强，点积结果就会很大，从而通过 ReLU 这个函数激活。一旦被激活了，MLP 的第二层线性层就会输出一个全新的向量。这个向量的方向代表了“篮球”这个概念。</p>
<p>最后一步就是残差的相加”：</p>
<script type="math/tex; mode=display">
\vec{E}_{new} = \vec{E}_{Michael+Jordan} + \Delta\vec{E}_{Basketball}</script><p>模型并没有把“乔丹”变成“篮球”，而是在“乔丹”的向量里添加了“篮球”这个属性。而经过这一层处理，这个向量就像变成了一个“带有篮球运动员标签的乔丹”。这个进化的向量会被传送到下一层，去寻找更多的关联（比如预测下一个词会不会是“灌篮”）。</p>
<p>所以在论文图中也有看到，其实在经过注意力和前馈神经网络（MLP）之后都有一个“Add”的操作，两者的区别在于Attention它打破了原本词向量间的孤立，让“乔丹”向量拿到了“迈克尔”的信息。MLP它识别出这个组合，产出对应的“事实”向量。而残差连接能把事实写回向量中。</p>
<p>注意力解决了 “谁和谁有关” 的问题。它把分散在序列各处的信息聚拢到一个向量里。MLP 解决了 “这些信息合在一起意味着什么” 的问题。它从聚拢的信息中提取出深层的、隐含的事实（比如“篮球”）。</p>
<p>其实MLP的结构和以前的传统神经网络很像，其实就是标准的三层架构：</p>
<ul>
<li>输入层：维度为 $d_{model}$。</li>
<li>隐藏层：通过线性变换（Linear）将维度拉伸到 $d_{ff}$，然后接一个非线性激活函数（ReLU 或 GELU）。</li>
<li>输出层：再通过一个线性变换缩回到 $d_{model}$。</li>
</ul>
<p>以前的神经网络通常尝试用一个巨大的MLP完成所有任务（识别、理解、输出）。Transformer 里的 MLP不再负责寻找词与词的关系，而是专心做存储。这也是为什么都在说MLP负责存储在训练中获得的知识。</p>
<p>具体来说，第一层就是一个线性层，就是传统 BP神经网络 的核心逻辑类似。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125100716255.png" alt="image-20260125100716255" style="zoom:50%;"></p>
<p>MLP 的内部计算公式是标准的 $W \vec{E}_i + \vec{B}$：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095520539.png" alt="image-20260125095520539" style="zoom:50%;"></p>
<p>然后就是传统的神经网络模型，用ReLu将线性表现为非线性：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095528883.png" alt="image-20260125095528883" style="zoom:50%;"></p>
<p>当然在大多数模型中，会用一个叫做GeLu(高斯误差线性单元)：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095540470.png" alt="image-20260125095540470" style="zoom:50%;"></p>
<p>因为ReLU (Rectified Linear Unit)的逻辑非常死板。只要点积结果是 $-0.00001$，输出就是 $0$；只要是正数，就原样输出。那么这样在训练中容易出现“神经元死亡”现象：如果一个神经元的偏置项变成了很大的负数，它可能永远无法被激活，导致这部分“内存”废掉了。而GeLu它结合了概率的思想。当输入是负数但接近 $0$ 时，它不会立刻切断信号，而是允许一点点带权重的负值信号（抑制信号）传过去。在处理“迈克尔·乔丹”这样的复杂语义时，GELU 的表现更像人类的模糊逻辑，现实中的向量信号不会像假设的“刚好是 1”那么完美，GELU 允许模型在信号稍有偏差或带有轻微噪音时，依然能保留一部分有用的信息，而不是直接把它杀掉。</p>
<p>经过这一部分的处理之后，最后一步是将这个向量降维至原始维度。在第一层（Up-projection）之后，向量被拉伸到了极高的维度（如 51,200 维），这是为了提供足够的检测空间。所以我们需要一个巨大的 $12,288 \times 51,200$ 的矩阵，把这 5 万多个神经元的输出“加权求和”，压缩回原始的 12,288 维。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125100933460.png" alt="image-20260125100933460" style="zoom:50%;"></p>
<p>在训练初期，这个矩阵里的数值也是随机的乱码。最初，当“迈克尔·乔丹”神经元被激活时，这个降维矩阵可能随机地输出一个代表“厨师”或“天空”的向量。模型发现预测错了，梯度信号就会顺着路径传回来，修改这个降维矩阵里对应那一列的数值。经过无数次修正，那一列的数值慢慢变成了一个指向篮球语义的向量。</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125101649720.png" alt="image-20260125101649720" style="zoom:50%;"></p>
<p>这个矩阵的列数和中间隐藏层的神经元数量一样多（比如 5 万列）。在这里，按列来看矩阵更符合知识提取的含义：</p>
<p>假设我们有一个矩阵 $A$（对应 MLP 的降维矩阵）和一个向量 $\vec{x}$（对应神经元的激活值）：</p>
<script type="math/tex; mode=display">
A\vec{x} = \begin{bmatrix} \vert & \vert & & \vert \\ \vec{C}_1 & \vec{C}_2 & \dots & \vec{C}_m \\ \vert & \vert & & \vert \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{bmatrix} = x_1\vec{C}_1 + x_2\vec{C}_2 + \dots + x_m\vec{C}_m</script><p>这个公式的意思是：矩阵 $A$ 乘以向量 $\vec{x}$，等于矩阵 $A$ 的各列向量以向量 $\vec{x}$ 的各个分量为系数进行线性组合。上图中可表示为：</p>
<script type="math/tex; mode=display">
\text{Output} = n_0\vec{C}_0 + n_1\vec{C}_1 + n_2\vec{C}_2 + \dots + \vec{B}</script><p>这里的每一个 $n_m$ 都是中间层神经元的激活值（也就是检测器的结果），而每一列 $\vec{C}_m$ 就是这个神经元对应的”结论向量”。更通俗一些可以理解为当多个神经元同时激活时，模型就把这些列向量按比例加在一起，比如：$0.8 \cdot [\text{篮球}] + 0.5 \cdot [\text{飞人}] + 0.2 \cdot [\text{棒球}]$，最终得到的 $\Delta\vec{E}$ 就是这些语义的“混合结论”。把这个矩阵记为$W\downarrow$偏置向量记为$B\downarrow$，然后把这些残差加回去，就表示模型学到了新的结论：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125103622914.png" alt="image-20260125103622914"></p>
<h1 id="GPT3的参数量计算"><a href="#GPT3的参数量计算" class="headerlink" title="GPT3的参数量计算"></a>GPT3的参数量计算</h1><p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124110107947.png" alt="image-20260124110107947" style="zoom: 50%;"></p>
<ol>
<li>词向量层 (Embedding &amp; Unembedding)，其实就是词典。</li>
</ol>
<ul>
<li>Embedding: 把文字（Token）变成初始向量。</li>
<li>Unembedding: 把最后算完的向量变回文字。</li>
<li>参数量: 取决于词库的大小（GPT-3 约有 5 万个词）和向量的维度（12,288 维）。这部分虽然重要，但在总参数量中占比很小。</li>
</ul>
<ol>
<li>注意力机制相关 (Key, Query, Value, Output)：GPT-3 有 96 层，每一层都有96 个注意力头。</li>
</ol>
<ul>
<li>Key, Query, Value ($W_K, W_Q, W_V$): 每一层、每一个头都有这三组矩阵。它们负责词与词之间的信息交换。</li>
<li>Output ($W_O$): 当 96 个头各自算出结果后，需要一个矩阵把这些不同的视角重新合并在一起。</li>
<li>每一层都有巨大的矩阵来处理 12,288 维的向量。<strong>因为每一层有96个头，所以每个头分配到的是12288/96=128,128×12288大小的$W_K, W_Q, W_V$</strong>。</li>
</ul>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260124114526262.png" alt="image-20260124114526262" style="zoom:50%;"></p>
<p>上文也有提到，为了符合矩阵运算$W_V$应该是12288×12288,但实际上会对大矩阵进行低秩分解，做了一个分割：</p>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125095036229.png" alt="image-20260125095036229" style="zoom:50%;"></p>
<p>但是实际实现中$Value\uparrow$和$Value\downarrow$不叫这个名字，所有的$Value\uparrow$会合在一起说成是输出矩阵，与整个注意力模块相关联，而每一个注意力头的$W_V$其实是$Value\downarrow$矩阵，即那些将嵌入向量投影到低纬度空间的矩阵。所以真正的$W_V$矩阵是12288（词向量大小）× 每一个注意力头需要的维度（12288/96=128）。Output ($W_O$)的维度则是他的转置矩阵的维度。</p>
<ol>
<li>前馈神经网络 (Up-projection &amp; Down-projection)，在每一层注意力计算完之后，模型会进入一个“全连接层”（Feed Forward）。</li>
</ol>
<ul>
<li>Up-projection: 把向量维度拉伸得非常大（GPT-3 将 12,288 维拉伸到 49,152 维，乘上4倍）。这是模型推理的地方。</li>
<li>Down-projection: 再把维度压缩回 12,288 维，传递给下一层。</li>
<li>这是模型中参数量最大的部分。大约 2/3 的参数都集中在这里。</li>
</ul>
<p><img src="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20260125104248008.png" alt="image-20260125104248008"></p>
<p>一个Token维度的变化：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>阶段</strong></th>
<th><strong>维度变化</strong></th>
<th><strong>具体的改变</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1. 初始输入</strong></td>
<td>$1 \to 12,288$</td>
<td><strong>查表嵌入（Embedding）</strong>。一个单词 ID 变成了一个 1.2 万维的稠密向量。</td>
</tr>
<tr>
<td><strong>2. 注意力层</strong></td>
<td>保持 $12,288$</td>
<td><strong>信息的交流</strong>。向量通过 Attention 吸收了周围词的信息，但体型没变。<br>具体来说，一开始是12,288 维的 $\vec{E}_i$。<br>然后被96个注意力头分身成 96 对 $\vec{Q}, \vec{K}, \vec{V}$，每组 128 维。<br>各组根据 $\vec{Q}$ 和 $\vec{K}$ 打分，确定注意力分数。<br>根据分数，并按照各自对应的$\vec{V}$ 加权求和，形成 96 个 128 维的$\Delta \vec{E}_i$。<br>把$\Delta \vec{E}_i$拼回 12288 维。<br>通过残差连接（Add），把这些情报加回原来的 $\vec{E}_i$ 上。</td>
</tr>
<tr>
<td><strong>3. MLP 升维</strong></td>
<td>$12,288 \to 49,152$</td>
<td><strong>升维（Up-projection）</strong>。向量被拉伸到近 5 万维，去触发那些“事实检测器”。</td>
</tr>
<tr>
<td><strong>4. 激活函数</strong></td>
<td>保持 $49,152$</td>
<td><strong>筛选（GELU）</strong>。5 万个维度中，只有少数符合逻辑的“神经元”保持活跃。</td>
</tr>
<tr>
<td><strong>5. MLP 降维</strong></td>
<td>$49,152 \to 12,288$</td>
<td><strong>降维（Down-projection）</strong>。把筛选出的新知识压缩回原始宽度。</td>
</tr>
<tr>
<td><strong>6. 循环往复</strong></td>
<td>重复以上过程 96 次</td>
<td><strong>迭代</strong>。GPT3设置了96层（n_layer参数），每一层都像这样升维再降维，向量就能获取更多的知识。</td>
</tr>
<tr>
<td><strong>7. 最终输出</strong></td>
<td>$12,288 \to 50,257$</td>
<td><strong>解密（Unembedding）</strong>。向量映射到 5 万个单词概率上，决定吐出哪个字。</td>
</tr>
</tbody>
</table>
</div>
<p>*其实这个对于GPT来说的<code>n_layer</code>参数在其他模型中叫做Decoder层的维度，传统 Transformer（如翻译任务）需要 Encoder 读入中文，Decoder 产出英文。GPT 系列是纯 Decoder 架构（Decoder-only），它本质上是一个“接龙”模型。它把之前写好的所有字都当成“已有的输入”，直接塞进一串 Decoder 里去预测下一个字。</p>
<h1 id="补充：关于TIme-Series-Library库的启动脚本的超参数解读"><a href="#补充：关于TIme-Series-Library库的启动脚本的超参数解读" class="headerlink" title="补充：关于TIme-Series-Library库的启动脚本的超参数解读"></a>补充：关于TIme-Series-Library库的启动脚本的超参数解读</h1><p><a target="_blank" rel="noopener" href="https://github.com/thuml/Time-Series-Library/tree/main">thuml/Time-Series-Library: A Library for Advanced Deep Time Series Models for General Time Series Analysis.</a></p>
<p>视频中涉及到的概念（$Q, K, V$, 多头注意、Embedding、参数量），在run.py里都有对应的超参数。</p>
<ul>
<li><strong><code>--d_model</code> </strong>:<ul>
<li>这就是一直在说的词向量维度 $d$。</li>
<li>它决定了每一个 Token 向量有多长。 $W_Q, W_K, W_V$ 矩阵的形状，就是由这个数字决定的。</li>
</ul>
</li>
<li><p><strong><code>--e_layers</code> / <code>--d_layers</code> </strong>:</p>
<ul>
<li>Encoder（编码器）和 Decoder（解码器）的层数。即信息交换与模型推理的地方。</li>
<li>GPT-3 有 96 层，且这96层全是解码器的层数。</li>
</ul>
</li>
<li><p><strong><code>--n_heads</code> </strong>:</p>
<ul>
<li>多头注意力（Multi-head Attention）。</li>
<li>这代表模型会同时开启 4 个头并行计算。每个头都有自己独立的 $W_Q, W_K, W_V$，分别去寻找不同的规律（比如头1找周期性，头2找趋势）。</li>
</ul>
</li>
<li><p><strong><code>--d_ff</code></strong>:</p>
<ul>
<li>前馈神经网络（Feed Forward）的维度。</li>
<li>这就是Up-projection。在注意力交换完情报后，向量会进入这个2048维的空间进行非线性变换。</li>
</ul>
</li>
<li><p><strong><code>--enc_in</code> </strong>:</p>
<ul>
<li>输入特征数。</li>
<li>这代表你的初始元素，对应了嵌入前的初始文本。比如有 7 种时间序列数据，Embedding 层会负责把这 7 个维度变成 <code>d_model</code>。</li>
</ul>
</li>
<li><p><strong><code>--embed</code> </strong>:</p>
<ul>
<li>对应Embedding 的方式。</li>
<li>这决定了模型如何把“时间”这个抽象概念编码进向量里。</li>
</ul>
</li>
<li><p><strong><code>--dropout</code> (0.1)</strong>: 在加权求和时，随机丢弃 10% 的神经元，防止模型过拟合。</p>
</li>
<li><strong><code>--activation</code> (‘gelu’)</strong>: 这是激活函数。在 $V$ 向量搬运完信息后，给“思考”过程增加一点非线性的逻辑，GPT 系列通常都是GELU。</li>
</ul>
<p>时间序列和文本任务在 Transformer 的应用上，存在几个核心的策略差异，文本任务（如 GPT）是目的是压缩，他文本原始维度是词表大小（5万+），d_model给的是1.2万，做特征降维和语义聚合。</p>
<p>但是时间序列，原始维度一般很小，达不到$10^4$级别，我做的数据集中最多也才26维。直接对 26 个维度做注意力计算，信息量太薄，模型很难学到复杂的非线性关系。因此一般会通过 Embedding 层将 26 维投影到 64 维。在时间序列论文（如 TimesNet, Informer, Autoformer）中，通常 d_model 设置为原始特征数的 2 到 4 倍 是比较稳妥的起点。所以在试的时候 d_model 从默认的 512 改成 64,48,32比较合适。当然这样做模型中所有 $W_Q, W_K, W_V$ 矩阵的面积缩小不少，跑的还是挺快的。</p>
<p>公式中$\sqrt{d_k}$ 的计算：在 <code>d_model=64</code>, <code>n_heads=4</code> 的情况下，每一个“头”分到的向量维度 $d_k$ 是16个，因为$d_k$ 代表Key向量的维度。那么由<code>d_model=64</code>得每一个头分到的是64/4=16个。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">z01prime</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="http://z01prime.github.io/2026/01/24/注意力机制/">http://z01prime.github.io/2026/01/24/注意力机制/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">z01prime</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2026/01/16/%E4%B8%89%E8%93%9D%E4%B8%80%E6%A3%95%EF%BC%9A%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" title="三蓝一棕：傅里叶变换"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">三蓝一棕：傅里叶变换</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2025/03/06/EPO2kshMSWTZoHe.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">z01prime</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/z01prime"><i class="fab fa-github"></i><span>我的github主页</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/z01prime" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:z7296091@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">加载慢可以挂个梯子试试捏</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">重要的概念</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">2.</span> <span class="toc-text">注意力机制的直观理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Token"><span class="toc-number">2.1.</span> <span class="toc-text">Token</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86Token%E5%85%B3%E8%81%94%E5%88%B0%E5%B5%8C%E5%85%A5%E5%90%91%E9%87%8F%E7%9A%84%E9%AB%98%E7%BB%B4%E5%90%91%E9%87%8F%E3%80%82"><span class="toc-number">2.2.</span> <span class="toc-text">将Token关联到嵌入向量的高维向量。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W-Q-%E7%9F%A9%E9%98%B5"><span class="toc-number">2.3.</span> <span class="toc-text">$W_Q$矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W-K-%E7%9F%A9%E9%98%B5"><span class="toc-number">2.4.</span> <span class="toc-text">$W_K$矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A9%E7%A0%81"><span class="toc-number">2.5.</span> <span class="toc-text">掩码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W-V-%E7%9F%A9%E9%98%B5"><span class="toc-number">2.6.</span> <span class="toc-text">$W_V$矩阵</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">多层感知机&#x2F;前馈神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GPT3%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="toc-number">4.</span> <span class="toc-text">GPT3的参数量计算</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%EF%BC%9A%E5%85%B3%E4%BA%8ETIme-Series-Library%E5%BA%93%E7%9A%84%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%E7%9A%84%E8%B6%85%E5%8F%82%E6%95%B0%E8%A7%A3%E8%AF%BB"><span class="toc-number">5.</span> <span class="toc-text">补充：关于TIme-Series-Library库的启动脚本的超参数解读</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="注意力机制">注意力机制</a><time datetime="2026-01-24T02:30:50.000Z" title="发表于 2026-01-24 10:30:50">2026-01-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/16/%E4%B8%89%E8%93%9D%E4%B8%80%E6%A3%95%EF%BC%9A%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" title="三蓝一棕：傅里叶变换">三蓝一棕：傅里叶变换</a><time datetime="2026-01-16T02:16:47.000Z" title="发表于 2026-01-16 10:16:47">2026-01-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/22/%E5%85%B3%E4%BA%8E%E9%9B%85%E6%80%9D/" title="关于雅思">关于雅思</a><time datetime="2025-12-22T02:29:43.000Z" title="发表于 2025-12-22 10:29:43">2025-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/21/2021SCCPC/" title="2021SCCPC">2021SCCPC</a><time datetime="2025-04-21T06:40:56.000Z" title="发表于 2025-04-21 14:40:56">2025-04-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/31/2024CCPC%E5%B1%B1%E4%B8%9C%E9%82%80%E8%AF%B7%E8%B5%9B/" title="2024CCPC山东邀请赛">2024CCPC山东邀请赛</a><time datetime="2024-12-31T07:43:10.000Z" title="发表于 2024-12-31 15:43:10">2024-12-31</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://source.fomal.cc/img/dm8.webp')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By z01prime</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'e8f61b4db601b696558e',
      clientSecret: '2f5cacaf3b111d5db02b1b40be93f02553bf9aa4',
      repo: 'commetForGitalk',
      owner: 'z01prime',
      admin: ['z01prime'],
      id: '654fa1289a125100ee5c061238a41edb',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.textContent= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="25" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>